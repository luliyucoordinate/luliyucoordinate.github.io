<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>深入研究MNIST | coordinate</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="pythontensorflowMNIST" />
  
  
    <meta name="google-site-verification" content="true" />
  
  
    <meta name="baidu-site-verification" content="true" />
  
  
    <meta name="360-site-verification" content="true" />
  
  <meta name="description" content="加载MNIST数据如果您正在复制和粘贴本教程的代码，请从这里开始，下面这两行代码将自动下载并读取数据： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True) 这里mnist是一个轻量级数据集，它将训练、验证和测">
<meta name="keywords" content="python,tensorflow,MNIST">
<meta property="og:type" content="article">
<meta property="og:title" content="深入研究MNIST">
<meta property="og:url" content="http://coordinate.wang/2017/12/03/2017-12-03-深入研究MNIST/index.html">
<meta property="og:site_name" content="coordinate">
<meta property="og:description" content="加载MNIST数据如果您正在复制和粘贴本教程的代码，请从这里开始，下面这两行代码将自动下载并读取数据： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True) 这里mnist是一个轻量级数据集，它将训练、验证和测">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm3dvcsajhj20c20qltan.jpg">
<meta property="og:updated_time" content="2018-02-18T08:38:31.599Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深入研究MNIST">
<meta name="twitter:description" content="加载MNIST数据如果您正在复制和粘贴本教程的代码，请从这里开始，下面这两行代码将自动下载并读取数据： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True) 这里mnist是一个轻量级数据集，它将训练、验证和测">
<meta name="twitter:image" content="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm3dvcsajhj20c20qltan.jpg">
  
    <link rel="alternate" href="/atom.xml" title="coordinate" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  


<header id="allheader" class="site-header" role="banner" 
   >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" rel="home" >
                <img style="margin-bottom: 10px;"  width="124px" height="124px" alt="Hike News" src=" /css/images/pose.jpg">
              </a>
            
          </h1>
          
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-2017-12-03-深入研究MNIST" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      深入研究MNIST
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/03/2017-12-03-深入研究MNIST/" class="article-date">
	  <time datetime="2017-12-02T16:00:00.000Z" itemprop="datePublished">十二月 3, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="加载MNIST数据"><a href="#加载MNIST数据" class="headerlink" title="加载MNIST数据"></a>加载MNIST数据</h2><p>如果您正在复制和粘贴本教程的代码，请从这里开始，下面这两行代码将自动下载并读取数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里mnist是一个轻量级数据集，它将训练、验证和测试集存储为NumPy数组。 它还提供了一个函数来迭代数据minibatches，我们将在下面使用。</p>
<a id="more"></a>
<h2 id="启动TensorFlow-InteractiveSession"><a href="#启动TensorFlow-InteractiveSession" class="headerlink" title="启动TensorFlow InteractiveSession"></a>启动TensorFlow InteractiveSession</h2><p>TensorFlow依靠高效的C ++后端来完成它的计算。 与此后端的连接称为会话。 TensorFlow程序的常见用法是首先创建一个图形，然后在会话中启动它。</p>
<p>在这里，我们使用便利的<code>InteractiveSession</code>类，这使得TensorFlow更加灵活地了解如何构建代码。 它允许您将构建计算图的操作与运行图的操作交错。 在IPython等交互式环境中工作时，这特别方便。 如果您没有使用<code>InteractiveSession</code>，则应在开始会话并启动图之前构建整个计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>为了在Python中进行有效的数值计算，我们通常使用像NumPy这样的库来执行复杂的操作，例如Python之外的矩阵乘法，使用另一种语言实现的高效代码来完成这些操作。不幸的是，每一次操作都会返回到Python，这仍然会有很多开销。如果要在GPU上运行计算或以分布式方式运行计算，则传输数据的成本很高，所以此开销尤其糟糕。</p>
<p>TensorFlow也在Python之外进行繁重的工作，但是为了避免这种开销，还需要进一步的工作。 TensorFlow不是独立于Python运行的一个复杂操作，而是让我们描述一个完全在Python之外运行的交互操作图。 （像这样的方法可以在几个机器学习库中看到）</p>
<p>因此，Python代码的作用是构建这个外部计算图，并指定运行图的每个部分。</p>
<p>#建立一个Softmax回归模型</p>
<p>在本节中，我们将建立一个单线性层的softmax回归模型。在下一节中，我们将扩展到具有多层卷积网络的softmax回归的情况。</p>
<h2 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h2><p>我们通过为输入图像和目标输出类创建节点来开始构建计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>这里<code>x</code>和<code>y_</code>不是特定的值。相反，它们都是占位符 - 当我们要求TensorFlow运行一个计算时，我们会输入一个值。</p>
<p>输入图像<code>x</code>将由浮点数的二维张量组成。这里我们给它赋予一个<code>[None，784]</code>的形状，其中<code>784</code>是一个单一的28×28像素MNIST图像的维度，<code>None</code>表示与batch大小相对应的第一维度可以是任意大小。目标输出类别<code>y_</code>也将由二维张量组成，其中每一行是一个唯一的10维向量，指示对应的MNIST图像是哪个数字（0到9）。</p>
<p>占位符的形状参数是可选的，但它允许TensorFlow自动捕捉源自不一致张量形状的错误。</p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>我们现在定义权重<code>W</code>，并为我们的模型赋予偏差<code>b</code>。我们可以把这些看作是额外的输入，但是TensorFlow有一个更好的方法来处理它们：Variable（变量）。变量是存在于TensorFlow的计算图中的一个值。它可以被使用，甚至被计算修改。在机器学习应用中，通常将模型参数设置为变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>我们将调用中的每个参数的初始值传递给<code>tf.Variable</code>。 在这种情况下，我们将<code>W</code>和<code>b</code>初始化为全0的张量。 W是784x10矩阵（因为我们有784个输入特征和10个输出），<code>b</code>是10维向量（因为我们有10个数字）。</p>
<p>在会话中使用变量之前，必须使用该会话进行初始化变量。 这一步将已经指定的初始值（在这里，张量全零），分配给每个变量。 这可以一次完成所有变量的赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<h2 id="预测类和损失函数"><a href="#预测类和损失函数" class="headerlink" title="预测类和损失函数"></a>预测类和损失函数</h2><p>我们现在可以实现我们的回归模型。 只需要一行！ 我们将向量化的输入图像<code>x</code>乘以权重矩阵<code>W</code>，加上偏差<code>b</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(x,W) + b</span><br></pre></td></tr></table></figure>
<p>我们可以很容易地指定一个损失函数。 损失函数描述模型的预测值在一个例子上有多差; 在训练所有的例子时，我们尽量减少损失函数的值。 在这里，我们的损失函数是目标和应用于模型预测的softmax激活函数之间的交叉熵。 正如在初学者教程中，我们使用同样的公式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</span><br></pre></td></tr></table></figure>
<p>请注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>在模型的非标准化模型预测中内部应用softmax，并在所有类中进行求和，<code>tf.reduce_mean</code>取这些和的平均值。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在我们已经定义了我们的模型和训练损失函数，接下来使用TensorFlow进行训练是很简单的。 由于TensorFlow知道整个计算图，因此可以使用自动微分来查找相对于每个变量的损失的梯度。 TensorFlow有多种内置的优化算法。 对于这个例子，我们将使用最陡的梯度下降，步长为0.5，降低交叉熵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>TensorFlow在这一行中实际上是在计算图中添加新的操作。 这些操作包括计算梯度、计算参数、更新步骤以及将更新步骤应用于参数。</p>
<p>运行<code>train_step</code>时返回的参数会用于更新梯度下降。 因此，训练模型可以通过重复运行<code>train_step</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>我们在每次训练迭代中加载100个训练样例。 然后我们运行<code>train_step</code>操作，使用<code>feed_dict</code>将训练样例中的占位符张量<code>x</code>和<code>y_</code>替换。 请注意，您可以使用<code>feed_dict</code>来替换计算图中的任何张量 - 它不仅限于占位符。</p>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>我们的模型有多好？</p>
<p>首先我们要弄清楚我们在哪里预测了正确的标签。 <code>tf.argmax</code>是一个非常有用的函数，它可以为您提供某个轴上张量的最大输入索引。 例如，<code>tf.argmax(y，1)</code>是我们模型认为对每个输入最有可能的标签，而<code>tf.argmax(y_，1)</code>是真正的标签。 我们可以使用<code>tf.equal</code>来检查我们的预测是否符合事实。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这里给出一个真值表。 为了确定什么分数是正确的，我们将布尔值转换为浮点数，然后取平均值。 例如，<code>[True，False，True，True]</code>将变成<code>[1,0,1,1]</code>，这将变为<code>0.75</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>最后，我们可以评估我们的测试数据的准确性。 这里应该是大约92％的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>#构建一个多层卷积网络</p>
<p>在MNIST上获得92％的准确性是不好的。 这几乎是令人尴尬的坏事。 在这一节中，我们将解决这个问题，从一个非常简单的模型跳到一些中等复杂的问题：一个小的卷积神经网络。 这将使我们达到约99.2％的准确性 - 不是最先进的，但是值得学习。</p>
<p>下面是一个用TensorBoard创建的关于我们将要构建的模型的图表：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm3dvcsajhj20c20qltan.jpg"><br></center>


<h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>要创建这个模型，我们需要创建很多权重和偏差。 一般应该用少量的噪声初始化权重，以防止对称性破坏，并防止0梯度。 由于我们使用的是ReLU神经元，为了避免“死神经元”，初始化这些神经元是一个很好的做法。 在我们构建模型的时候不要重复这样的操作，而是创建两个函数来为我们做这件事。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h2 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h2><p>TensorFlow也为卷积和池化操作提供了很大的灵活性。 我们如何处理边界？ 我们的步幅是多少？ 在这个例子中，我们选择vanilla版本。 我们使步幅大小为1，并在周围填充零，以便输出与输入大小相同。 我们的pooling是超过2x2的max pooling。 为了保证我们的代码更清晰，我们也将这些操作抽象为函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<p>tf.nn.conv2d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=<span class="keyword">True</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>input：张量。必须是以下类型之一：half，float32。一个四维张量。维度顺序根据data_format的值来解释，详见下文。</li>
<li>filter：张量。必须具有与输入相同的类型。形状的四维张量[filter_height，filter_width，in_channels，out_channels]</li>
<li>strides：整数列表。一维长度的张量4.输入每个维度的滑动窗口的步幅。维度顺序由data_format的值决定，详见下文。</li>
<li>padding：来自“SAME”，“VALID”的字符串。要使用的填充算法的类型。</li>
<li>use_cudnn_on_gpu：一个可选的布尔。默认为True。</li>
<li>data_format：来自“NHWC”，“NCHW”的可选字符串。默认为“NHWC”。指定输入和输出数据的数据格式。使用默认格式“NHWC”，数据按照[batch，height，width，channels]的顺序存储。或者，格式可以是“NCHW”，数据存储顺序为：[batch，channels，height，width]。</li>
<li>name：操作的名称（可选）。</li>
</ul>
<p>tf.nn.max_pool</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">max_pool(</span><br><span class="line">    value,</span><br><span class="line">    ksize,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>value：由data_format指定的格式的4维张量。</li>
<li>ksize：4元素的1维 int型张量。 输入张量表示窗口每个维度的大小。</li>
<li>strides：4元素的1维int型张量。 输入张量表示滑动窗口每个维度的步幅。</li>
<li>padding：一个字符串，可以是’VALID’ 或 ‘SAME’。</li>
<li>data_format：一个字符串。 支持“NHWC”，“NCHW”和“NCHW_VECT_C”。</li>
<li>name：操作的可选名称。</li>
</ul>
<h2 id="第一卷积层"><a href="#第一卷积层" class="headerlink" title="第一卷积层"></a>第一卷积层</h2><p>我们现在可以实现我们的第一层。 它将由卷积组成，然后是max pooling。 卷积将为每个5x5 patch计算32个特征。 它的权重张量是<code>[5,5,1,32]</code>的形状。 前两个维度是patch大小，下一个是输入通道的数量，最后一个是输出通道的数量。 每个输出通道还会有带有一个偏差向量的分量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br></pre></td></tr></table></figure>
<p>为了应用该层，我们首先将<code>x</code>重塑为4维张量，第二维和第三维对应于图像的宽度和高度，并且最后一个维度对应于色彩通道的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>然后，我们将<code>x_image</code>与权重张量进行卷积，加上偏差，应用ReLU函数，最后使用max pooling。 <code>max_pool_2x2</code>方法将图像大小减小到14x14。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br></pre></td></tr></table></figure>
<h2 id="第二卷积层"><a href="#第二卷积层" class="headerlink" title="第二卷积层"></a>第二卷积层</h2><p>为了建立一个深层网络，我们堆叠了这种类型的几个层。 第二层将为每个5x5 patch有64个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h2 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h2><p>现在图像尺寸已经减小到7x7，我们添加了一个1024个神经元的全连接图层，以允许在整个图像上进行处理。 我们将pooling层中的张量重塑为一批向量，乘以权重矩阵，添加一个偏差，并应用一个ReLU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>为了减少过拟合，我们将在读出层之前应用dropout。 我们创建一个占位符，用于在dropout期间保持神经元输出的概率。 这可以让我们在训练过程中关闭dropout，并在测试过程中将其关闭。 TensorFlow的<code>tf.nn.dropout</code>可以自动处理缩放神经元输出和掩蔽它们，所以dropout只是在没有任何附加缩放的情况下工作。（对于这个小卷积网络，性能实际上几乎是相同的，没有丢失。 dropout对于减少过拟合通常是非常有效的，但是是在训练非常大的神经网络。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<p>tf.nn.dropout</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropout(</span><br><span class="line">    x,</span><br><span class="line">    keep_prob,</span><br><span class="line">    noise_shape=<span class="keyword">None</span>,</span><br><span class="line">    seed=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>x：浮点张量。</li>
<li>keep_prob：与x相同类型的张量。 每个元素被保留的概率。</li>
<li>noise_shape：int32类型的一维张量，表示随机生成的保留/丢弃标志。</li>
<li>seed：一个Python整数。 用于创建随机种子。 有关行为，请参阅tf.set_random_seed。</li>
<li>name：此操作的名称（可选）。</li>
</ul>
<p>对于概率<code>keep_prob</code>，输出按1 / keep_prob放大的输入元素，否则输出0。</p>
<h2 id="读出层"><a href="#读出层" class="headerlink" title="读出层"></a>读出层</h2><p>最后，我们添加一个图层，就像上面的一层softmax回归一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<h2 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h2><p>这个模型有多好？ 为了训练和评估，我们将使用与上述简单的一层SoftMax网络几乎相同的代码。</p>
<p>不同之处在于：</p>
<ul>
<li>我们将用更复杂的ADAM优化器替代最陡的梯度下降优化器。</li>
<li>我们将在<code>feed_dict</code>中包含附加参数<code>keep_prob</code>来控制丢失率。</li>
<li>我们将在训练过程中每100次迭代添加一次记录。</li>
</ul>
<p>我们也将使用<code>tf.Session</code>而不是<code>tf.InteractiveSession</code>。 这更好地分离了创建图（模型说明）的过程和评估图（模型拟合）的过程。 它通常使更清晰的代码。<code>tf.Session</code>是在一个块内创建的，所以一旦块退出，它就会被自动销毁。</p>
<p>运行这个代码。 请注意，它会进行20,000次训练迭代，可能需要一段时间（可能长达半小时），具体取决于您的处理器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">          x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">      print(<span class="string">'step %d, training accuracy %g'</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">'test accuracy %g'</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">      x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>运行此代码后的最终测试集精度应该约为99.2％。</p>
<p>我们已经学会了如何使用TensorFlow快速，轻松地构建，训练和评估相当复杂的深度学习模型。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MNIST/">MNIST</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
        
	<section id="comments" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'luliyucoordinate';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/12/04/2017-12-04-tf.estimator-快速上手/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          tf.estimator 快速上手
        
      </div>
    </a>
  
  
    <a href="/2017/12/02/2017-12-02-MNIST对于机器学习的初学者/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">Tensorflow学习始于MNIST</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#加载MNIST数据"><span class="nav-number">1.</span> <span class="nav-text">加载MNIST数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#启动TensorFlow-InteractiveSession"><span class="nav-number">2.</span> <span class="nav-text">启动TensorFlow InteractiveSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算图"><span class="nav-number">3.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#占位符"><span class="nav-number">4.</span> <span class="nav-text">占位符</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变量"><span class="nav-number">5.</span> <span class="nav-text">变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预测类和损失函数"><span class="nav-number">6.</span> <span class="nav-text">预测类和损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练模型"><span class="nav-number">7.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#评估模型"><span class="nav-number">8.</span> <span class="nav-text">评估模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#权重初始化"><span class="nav-number">9.</span> <span class="nav-text">权重初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积和池化"><span class="nav-number">10.</span> <span class="nav-text">卷积和池化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第一卷积层"><span class="nav-number">11.</span> <span class="nav-text">第一卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二卷积层"><span class="nav-number">12.</span> <span class="nav-text">第二卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#密集连接层"><span class="nav-number">13.</span> <span class="nav-text">密集连接层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout"><span class="nav-number">14.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#读出层"><span class="nav-number">15.</span> <span class="nav-text">读出层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练和评估模型"><span class="nav-number">16.</span> <span class="nav-text">训练和评估模型</span></a></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2018 coordinate All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
