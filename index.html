<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第 1 页 | coordinate</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="coordinate">
<meta property="og:url" content="http://coordinate.wang/index.html">
<meta property="og:site_name" content="coordinate">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="coordinate">
  
    <link rel="alternate" href="/atom.xml" title="coordinate" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>



  <body>


  


<header id="allheader" class="site-header" role="banner" 
   >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="coordinate" rel="home"> coordinate </a>
            
          </h1>
          
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main">
  
    <article id="post-2017-12-20-YOLOv2代码分析（四）"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/20/2017-12-20-YOLOv2代码分析（四）/">YOLOv2代码分析（四）</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/20/2017-12-20-YOLOv2代码分析（四）/" class="article-date">
	  <time datetime="2017-12-19T16:00:00.000Z" itemprop="datePublished">十二月 20, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0x01-backward-convolutional-layer"><a href="#0x01-backward-convolutional-layer" class="headerlink" title="0x01 backward_convolutional_layer"></a>0x01 backward_convolutional_layer</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_convolutional_layer</span><span class="params">(convolutional_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="keyword">int</span> m = l.n/l.groups;				<span class="comment">//每组卷积核的个数</span></span><br><span class="line">    <span class="keyword">int</span> n = l.size*l.size*l.c/l.groups;	<span class="comment">//每组卷积核的元素个数</span></span><br><span class="line">    <span class="keyword">int</span> k = l.out_w*l.out_h;			<span class="comment">//输出图像的元素个数</span></span><br><span class="line"></span><br><span class="line">    gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta);</span><br></pre></td></tr></table></figure>
<p>这里出现了一个<code>gradient_array</code>函数，我们看看这个函数有什么作用。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">gradient</span><span class="params">(<span class="keyword">float</span> x, ACTIVATION a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(a)&#123;</span><br><span class="line">        <span class="keyword">case</span> LINEAR:</span><br><span class="line">            <span class="keyword">return</span> linear_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> LOGISTIC:</span><br><span class="line">            <span class="keyword">return</span> logistic_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> LOGGY:</span><br><span class="line">            <span class="keyword">return</span> loggy_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> RELU:</span><br><span class="line">            <span class="keyword">return</span> relu_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> ELU:</span><br><span class="line">            <span class="keyword">return</span> elu_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> RELIE:</span><br><span class="line">            <span class="keyword">return</span> relie_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> RAMP:</span><br><span class="line">            <span class="keyword">return</span> ramp_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> LEAKY:</span><br><span class="line">            <span class="keyword">return</span> leaky_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> TANH:</span><br><span class="line">            <span class="keyword">return</span> tanh_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> PLSE:</span><br><span class="line">            <span class="keyword">return</span> plse_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> STAIR:</span><br><span class="line">            <span class="keyword">return</span> stair_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> HARDTAN:</span><br><span class="line">            <span class="keyword">return</span> hardtan_gradient(x);</span><br><span class="line">        <span class="keyword">case</span> LHTAN:</span><br><span class="line">            <span class="keyword">return</span> lhtan_gradient(x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gradient_array</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> *x, <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> ACTIVATION a, <span class="keyword">float</span> *delta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">        delta[i] *= gradient(x[i], a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">linear_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> <span class="number">1</span>;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">logistic_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (<span class="number">1</span>-x)*x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">relu_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>);&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">elu_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x &gt;= <span class="number">0</span>) + (x &lt; <span class="number">0</span>)*(x + <span class="number">1</span>);&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">relie_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>) ? <span class="number">1</span> : <span class="number">.01</span>;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">ramp_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>)+<span class="number">.1</span>;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">leaky_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>) ? <span class="number">1</span> : <span class="number">.1</span>;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">tanh_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> <span class="number">1</span>-x*x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">plse_gradient</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x &lt; <span class="number">0</span> || x &gt; <span class="number">1</span>) ? <span class="number">.01</span> : <span class="number">.125</span>;&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用很明显，就是将<code>layer</code>的输出图像，输入到相应的梯度下降算法（计算每一个元素对应激活函数的导数），最后将<code>delta</code>的每个元素乘以激活函数的导数，结果送到<code>delta</code>指向的内存中。</p>
<p>举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m=2 n=3x3=9 k=3x3=9</span><br><span class="line">x [95 107 107 95]</span><br><span class="line">relu激活</span><br><span class="line">delta[0]=95 delta[1]=107 delta[2]=107 delta[3]=105</span><br></pre></td></tr></table></figure>
<p>接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">       backward_batchnorm_layer(l, net);</span><br></pre></td></tr></table></figure>
<p>出现这个<code>backward_batchnorm_layer</code>函数</p>
<h2 id="0x0101-backward-batchnorm-layer"><a href="#0x0101-backward-batchnorm-layer" class="headerlink" title="0x0101 backward_batchnorm_layer"></a>0x0101 backward_batchnorm_layer</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_batchnorm_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!net.train)&#123;</span><br><span class="line">        l.mean = l.rolling_mean;</span><br><span class="line">        l.variance = l.rolling_variance;</span><br><span class="line">    &#125;</span><br><span class="line">    backward_bias(l.bias_updates, l.delta, l.batch, l.out_c, l.out_w*l.out_h);</span><br><span class="line">    backward_scale_cpu(l.x_norm, l.delta, l.batch, l.out_c, l.out_w*l.out_h, l.scale_updates);</span><br><span class="line"></span><br><span class="line">    scale_bias(l.delta, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line"></span><br><span class="line">    mean_delta_cpu(l.delta, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.mean_delta);</span><br><span class="line">    variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.variance_delta);</span><br><span class="line">    normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    <span class="keyword">if</span>(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, l.delta, <span class="number">1</span>, net.delta, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里面也有很多函数，我们一一分析</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_bias</span><span class="params">(<span class="keyword">float</span> *bias_updates, <span class="keyword">float</span> *delta, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,b;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">            bias_updates[i] += sum_array(delta+size*(i+b*n), size);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">sum_array</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">int</span> n)</span><span class="comment">//计算输入数组的和</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i) sum += a[i];</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>bias_updates</code>:指向权重更新的指针</li>
<li><code>delta</code>:指向前面梯度下降得到的累乘值</li>
<li><code>batch</code>:<code>batch</code>大小</li>
<li><code>n</code>:<code>layer</code>的输出通道数目</li>
<li><code>size</code>:输出图像的元素个数</li>
</ul>
<p>这个函数就是计算偏置的更新值（误差函数对偏置的导数），将<code>delta</code>对应同一个卷积核的项相加。</p>
<p>这个函数想要描述的就是这个公式</p>
<ul>
<li>$\frac{∂L}{∂\beta}=\sum_0^m\frac{∂L}{∂y_i}$</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_scale_cpu</span><span class="params">(<span class="keyword">float</span> *x_norm, <span class="keyword">float</span> *delta, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> size, <span class="keyword">float</span> *scale_updates)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,b,f;</span><br><span class="line">    <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; n; ++f)&#123;</span><br><span class="line">        <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; ++i)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = i + size*(f + n*b);</span><br><span class="line">                sum += delta[index] * x_norm[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        scale_updates[f] += sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数想要描述的就是这个公式</p>
<ul>
<li>$\frac{∂L}{∂\gamma}=\sum_0^m\frac{∂l}{∂y_i}\hat{x_i}$</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mean_delta_cpu</span><span class="params">(<span class="keyword">float</span> *delta, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial, <span class="keyword">float</span> *mean_delta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> i,j,k;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean_delta[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; batch; ++j) &#123;</span><br><span class="line">            <span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; spatial; ++k) &#123;</span><br><span class="line">                <span class="keyword">int</span> index = j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean_delta[i] += delta[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean_delta[i] *= (<span class="number">-1.</span>/<span class="built_in">sqrt</span>(variance[i] + <span class="number">.00001</span>f));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数想要描述的就是这个公式</p>
<ul>
<li>$\frac{∂L}{∂mean}=\sum_0^m\frac{∂L}{∂y_i}\frac{∂y_i}{∂mean}=\sum_0^m\frac{∂L}{∂y_i}\frac{−1}{\sqrt{var+eps}}$</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span>  <span class="title">variance_delta_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *delta, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial, <span class="keyword">float</span> *variance_delta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> i,j,k;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance_delta[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; batch; ++j)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; spatial; ++k)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = j*filters*spatial + i*spatial + k;</span><br><span class="line">                variance_delta[i] += delta[index]*(x[index] - mean[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance_delta[i] *= <span class="number">-.5</span> * <span class="built_in">pow</span>(variance[i] + <span class="number">.00001</span>f, (<span class="keyword">float</span>)(<span class="number">-3.</span>/<span class="number">2.</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数想要描述的就是这个公式</p>
<ul>
<li>$\frac{∂L}{∂var}=\sum_0^m\frac{∂L}{∂y_i}(x_i−mean)(-\frac{1}{2})(var+eps)^{−\frac{3}{2}}$</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_delta_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">float</span> *mean_delta, <span class="keyword">float</span> *variance_delta, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial, <span class="keyword">float</span> *delta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> f, j, k;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; batch; ++j)&#123;</span><br><span class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; spatial; ++k)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = j*filters*spatial + f*spatial + k;</span><br><span class="line">                delta[index] = delta[index] * <span class="number">1.</span>/(<span class="built_in">sqrt</span>(variance[f] + <span class="number">.00001</span>f)) + variance_delta[f] * <span class="number">2.</span> * (x[index] - mean[f]) / (spatial * batch) + mean_delta[f]/(spatial*batch);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数想要描述的就是这个公式</p>
<ul>
<li>$\frac{∂L}{∂x_i}=\frac{∂L}{∂y_i}\frac{1}{\sqrt{var+eps}}+\frac{∂L}{∂var}\frac{∂var}{∂x_i}+\frac{∂L}{∂mean}\frac{∂mean}{∂x_i}=\frac{∂L}{∂y_i}\frac{1}{\sqrt{var+eps}}+\frac{∂L}{∂var}\frac{2}{m}(x_i-mean)+\frac{∂L}{∂mean}\frac{1}{m}$</li>
</ul>
<p>回到<code>backward_convolutional_layer</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//backward_convolutional_layer 	</span></span><br><span class="line">	<span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        backward_batchnorm_layer(l, net);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        backward_bias(l.bias_updates, l.delta, l.batch, l.n, k);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>如果没有定义<code>batch_normalize</code>的话，直接更新<code>bias</code>就完事了。</p>
<p>接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">	<span class="comment">//backward_convolutional_layer</span></span><br><span class="line">	<span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; l.groups; ++j)&#123;</span><br><span class="line">            <span class="keyword">float</span> *a = l.delta + (i*l.groups + j)*m*k;</span><br><span class="line">            <span class="keyword">float</span> *b = net.workspace;</span><br><span class="line">            <span class="keyword">float</span> *c = l.weight_updates + j*l.nweights/l.groups;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> *im = net.input+(i*l.groups + j)*l.c/l.groups*l.h*l.w;</span><br><span class="line"></span><br><span class="line">            im2col_cpu(im, l.c/l.groups, l.h, l.w, </span><br><span class="line">                    l.size, l.stride, l.pad, b);</span><br><span class="line">            gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(net.delta)&#123;</span><br><span class="line">                a = l.weights + j*l.nweights/l.groups;</span><br><span class="line">                b = l.delta + (i*l.groups + j)*m*k;</span><br><span class="line">                c = net.workspace;</span><br><span class="line"></span><br><span class="line">                gemm(<span class="number">1</span>,<span class="number">0</span>,n,k,m,<span class="number">1</span>,a,n,b,k,<span class="number">0</span>,c,k);</span><br><span class="line"></span><br><span class="line">                col2im_cpu(net.workspace, l.c/l.groups, l.h, l.w, l.size, l.stride, </span><br><span class="line">                    l.pad, net.delta + (i*l.groups + j)*l.c/l.groups*l.h*l.w);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先我们这里先说明一个问题，细心的同学会发现，这里使用的是<code>gemm(0,1...)</code>，也就是我在（三）中说过的，我这里对<code>B</code>进行了转置操作。为什么要这样做呢？</p>
<p>先看看参数是什么意思。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> m = l.n/l.groups;				<span class="comment">//每组卷积核的个数</span></span><br><span class="line"><span class="keyword">int</span> n = l.size*l.size*l.c/l.groups;	<span class="comment">//每组卷积核的元素个数</span></span><br><span class="line"><span class="keyword">int</span> k = l.out_w*l.out_h;			<span class="comment">//输出图像的元素个数</span></span><br></pre></td></tr></table></figure>
<p><code>a</code>指向一个<code>group</code>的<code>l.delta</code>的一行，元素个数为<code>(l.out_c)*(l.out_h*l.out_w)</code>；<code>b</code>指向保存结果的内存，大小是<code>(l.c*l.size*l.size)*(l.out_h*l.out_w)</code>；<code>c</code>指向一个<code>group</code>的<code>weight</code>的一行，大小是<code>(l.n)*(l.c*l.size*l.size)</code>。<code>gemm</code>描述的是这样一种运算<code>a*b+c</code>。所以根据矩阵运算的原理，这里的<code>b</code>要进行转置操作。</p>
<p>那么这里的卷积作用也就非常明显了，就是就算当前层的权重更新<code>c=alpha*a*b+beta*c</code>。</p>
<p>和之前的<code>forward_convolutional_layer</code>函数参数对比</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> m = l.n/l.groups;<span class="comment">//一个group的卷积核个数</span></span><br><span class="line"><span class="keyword">int</span> k = l.size*l.size*l.c/l.groups;<span class="comment">//一个group的卷积核元素个数</span></span><br><span class="line"><span class="keyword">int</span> n = l.out_w*l.out_h;<span class="comment">//一个输出图像的元素个数</span></span><br><span class="line"><span class="keyword">float</span> *a = l.weights + j*l.nweights/l.groups;</span><br><span class="line"><span class="keyword">float</span> *b = net.workspace;</span><br><span class="line"><span class="keyword">float</span> *c = l.output + (i*l.groups + j)*n*m;</span><br></pre></td></tr></table></figure>
<p>接着看后面这个判断语句中的内容</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//backward_convolutional_layer</span></span><br><span class="line"><span class="keyword">if</span>(net.delta)&#123;</span><br><span class="line">    a = l.weights + j*l.nweights/l.groups;<span class="comment">//注意此时权重没有更新，我们上面算的是放在了weight_updates里面</span></span><br><span class="line">    b = l.delta + (i*l.groups + j)*m*k;</span><br><span class="line">    c = net.workspace;</span><br><span class="line"></span><br><span class="line">    gemm(<span class="number">1</span>,<span class="number">0</span>,n,k,m,<span class="number">1</span>,a,n,b,k,<span class="number">0</span>,c,k);</span><br><span class="line"></span><br><span class="line">    col2im_cpu(net.workspace, l.c/l.groups, l.h, l.w, l.size, l.stride, </span><br><span class="line">               l.pad, net.delta + (i*l.groups + j)*l.c/l.groups*l.h*l.w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们看看这里的<code>gemm</code>和前面的有什么区别，首先看参数。这里的<code>a</code>对应上面的<code>c</code>，<code>b</code>对应上面的<code>a</code>，<code>c</code>对应上面的<code>b</code>。</p>
<p><code>a</code>指向<code>weight</code>，大小是<code>(l.n)*(l.c*l.size*l.size)</code>;<code>b</code>指向<code>delta</code>，大小是<code>(l.out_c)*(l.out_h*l*out_w)</code>；<code>c</code>指向输出存储空间，大小是<code>(l.c*l.size*l.size)*(l.out_h*l.out_w)</code>。那么这里调用<code>gemm(1,0...)</code>就很好理解了，最后完成<code>c=alpha*a*b+beta*c</code>操作，也就是更新<code>workspace</code>的工作。</p>
<p>那么这里这个函数到底有什么意义呢？</p>
<p>通过上面那个<code>c=alpha*a*b+beta*c</code>给我们的直观感受，就是将当前层的<code>delta</code>和上一层<code>weights</code>进行卷积操作，这个得到的结果意义不是很大，但是他经过之前说的<code>gradient_array</code>操作后就有了非常重要的意义，就是上一层的<code>delta</code>。为什么？这就要从<code>bp</code>算法开始说了</p>
<h2 id="0x02-卷积层误差传递"><a href="#0x02-卷积层误差传递" class="headerlink" title="0x02 卷积层误差传递"></a>0x02 卷积层误差传递</h2><p>首先举个例子</p>
<center class="half"><br><img src="http://wx2.sinaimg.cn/mw690/af2d2659ly1fmogsxbpjvj20sd0943yh.jpg"><br></center>

<p>我们假设输入是<code>A</code>，卷积核是<code>W</code>，输出是<code>C</code>，激活函数是<code>f</code>，偏向是<code>B</code>，我们可以知道</p>
<ul>
<li>$out=W*A+B$</li>
</ul>
<ul>
<li>$C=f(out)​$</li>
</ul>
<p>我们假设损失函数是<code>L</code>，那么可以计算出误差项$\Delta$</p>
<ul>
<li>$\Delta=\frac{∂L}{∂out}$</li>
</ul>
<p>好的现在我们要求解<code>l-1</code>层的$\Delta$</p>
<ul>
<li>$\Delta^{l-1}=\frac{∂L}{∂out^{l-1}}=\frac{∂L}{∂C^{l-1}}\frac{∂C^{l-1}}{∂out^{l-1}}=\frac{∂L}{∂C^{l-1}}f^\prime(out^{l-1})$</li>
</ul>
<center class="half"><br><img src="http://wx3.sinaimg.cn/mw690/af2d2659ly1fmogsxrz62j20rw0bldft.jpg"><br></center>


<p>由上面这个图不难看出</p>
<ul>
<li>$\frac{∂L}{∂C^{l-1}}=\Delta^l*W$</li>
</ul>
<p>所以</p>
<ul>
<li>$\Delta^{l-1}=\Delta^l*W\circ f^\prime(out^{l-1})$</li>
</ul>
<p>回到<code>backward_convolutional_layer</code>这个函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//backward_convolutional_layer</span></span><br><span class="line">col2im_cpu(net.workspace, l.c/l.groups, l.h, l.w, l.size, l.stride, </span><br><span class="line">              l.pad, net.delta + (i*l.groups + j)*l.c/l.groups*l.h*l.w);</span><br></pre></td></tr></table></figure>
<p>最后这个函数<code>col2im_cpu</code>的作用就是将<code>net.workspace</code>重排，类似于（三）中的<code>im2col_cpu</code>，只是这里反过来了。</p>
<h1 id="0x03-update-convolutional-layer"><a href="#0x03-update-convolutional-layer" class="headerlink" title="0x03 update_convolutional_layer"></a>0x03 update_convolutional_layer</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">update_convolutional_layer</span><span class="params">(convolutional_layer l, update_args a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> learning_rate = a.learning_rate*l.learning_rate_scale;</span><br><span class="line">    <span class="keyword">float</span> momentum = a.momentum;</span><br><span class="line">    <span class="keyword">float</span> decay = a.decay;</span><br><span class="line">    <span class="keyword">int</span> batch = a.batch;</span><br><span class="line"></span><br><span class="line">    axpy_cpu(l.n, learning_rate/batch, l.bias_updates, <span class="number">1</span>, l.biases, <span class="number">1</span>);</span><br><span class="line">    scal_cpu(l.n, momentum, l.bias_updates, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(l.scales)&#123;</span><br><span class="line">        axpy_cpu(l.n, learning_rate/batch, l.scale_updates, <span class="number">1</span>, l.scales, <span class="number">1</span>);</span><br><span class="line">        scal_cpu(l.n, momentum, l.scale_updates, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    axpy_cpu(l.nweights, -decay*batch, l.weights, <span class="number">1</span>, l.weight_updates, <span class="number">1</span>);</span><br><span class="line">    axpy_cpu(l.nweights, learning_rate/batch, l.weight_updates, <span class="number">1</span>, l.weights, <span class="number">1</span>);</span><br><span class="line">    scal_cpu(l.nweights, momentum, l.weight_updates, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数很容易理解了，就是用来更新网络参数的。其中<code>axpy_cpu</code>和<code>scal_cpu</code>这两个函数我在（三）中也讲过。</p>
<p>至此这三个重要的函数就分析完了，下一章我们会回到<code>make_convolutional_layer</code>函数</p>
<p>文章全部<a href="http://blog.csdn.net/column/details/18380.html" target="_blank" rel="noopener">YOLOv2源码分析</a></p>
<p>由于本人水平有限，文中有不对之处，希望大家指出，谢谢^_^!</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c/">c</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/darknet/">darknet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-19-YOLOv2代码分析（三）"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/19/2017-12-19-YOLOv2代码分析（三）/">YOLOv2代码分析（三）</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/19/2017-12-19-YOLOv2代码分析（三）/" class="article-date">
	  <time datetime="2017-12-18T16:00:00.000Z" itemprop="datePublished">十二月 19, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>接着上一讲没有讲完的<code>make_convolutional_layer</code>函数</p>
<h1 id="0x01-make-convolutional-layer"><a href="#0x01-make-convolutional-layer" class="headerlink" title="0x01 make_convolutional_layer"></a>0x01 make_convolutional_layer</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//make_convolutional_layer</span></span><br><span class="line">l.forward = forward_convolutional_layer;</span><br><span class="line">   l.backward = backward_convolutional_layer;</span><br><span class="line">   l.update = update_convolutional_layer;</span><br></pre></td></tr></table></figure>
<p>上来就是三坐大山^_^，我们先从第一个<code>forward_convolutional_layer</code>开始。</p>
<h2 id="0x0101-forward-convolutional-layer"><a href="#0x0101-forward-convolutional-layer" class="headerlink" title="0x0101 forward_convolutional_layer"></a>0x0101 forward_convolutional_layer</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_convolutional_layer</span><span class="params">(convolutional_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;<span class="comment">//传入卷积层参数和网络的总参数</span></span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line"></span><br><span class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>看这个<code>fill_cpu</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fill_cpu</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">float</span> ALPHA, <span class="keyword">float</span> *X, <span class="keyword">int</span> INCX)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; ++i) X[i*INCX] = ALPHA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输入的参数<code>N</code>表示一个<code>batch</code>中所有的图像元素个数，<code>x</code>指向<code>n</code>对应大小分配的内存空间。整个函数来看就是对输出图像元素的一个初始化操作。</p>
<p>接着看后面</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//forward_convolutional_layer</span></span><br><span class="line"><span class="keyword">if</span>(l.xnor)&#123;</span><br><span class="line">       binarize_weights(l.weights, l.n, l.c/l.groups*l.size*l.size, l.binary_weights);</span><br><span class="line">       swap_binary(&amp;l);</span><br><span class="line">       binarize_cpu(net.input, l.c*l.h*l.w*l.batch, l.binary_input);</span><br><span class="line">       net.input = l.binary_input;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>判断是否二值化操作，如果是的话，其中有两个关键的函数<code>binarize_weights</code>和<code>binarize_cpu</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">binarize_weights</span><span class="params">(<span class="keyword">float</span> *weights, <span class="keyword">int</span> n, <span class="keyword">int</span> size, <span class="keyword">float</span> *binary)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, f;</span><br><span class="line">    <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; n; ++f)&#123;</span><br><span class="line">        <span class="keyword">float</span> mean = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; ++i)&#123;</span><br><span class="line">            mean += <span class="built_in">fabs</span>(weights[f*size + i]);</span><br><span class="line">        &#125;</span><br><span class="line">        mean = mean / size;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; ++i)&#123;</span><br><span class="line">            binary[f*size + i] = (weights[f*size + i] &gt; <span class="number">0</span>) ? mean : -mean;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一个参数就是指向分配给<code>weight</code>内存空间 的指针，第二参数是卷积核个数，第三个参数是一个卷积核<code>weight</code>的个数（这里应该使用<code>l.nweights</code>/<code>l.n</code>），第四个参数是指向分配给二值化<code>weight</code>内存空间 的指针。举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">假设有两个2x2卷积核</span><br><span class="line">n=2  size=4</span><br><span class="line">权重值总共8个 1 2 3 4 5 6 7 8</span><br><span class="line"></span><br><span class="line">第一次循环 f=0 </span><br><span class="line">mean = 1+2+3+4 = 10</span><br><span class="line">mean/4 = 2.5</span><br><span class="line">binary[0]=2.5 binary[1]=2.5 binary[2]=2.5 binary[3]=2.5</span><br><span class="line"></span><br><span class="line">第二次循环 f=1</span><br><span class="line">mean = 5+6+7+8 = 26</span><br><span class="line">mean/4 = 6.5</span><br><span class="line">binary[0]=6.5 binary[1]=6.5 binary[2]=6.5 binary[3]=6.5</span><br></pre></td></tr></table></figure>
<p>接着看后面的<code>swap_binary</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap_binary</span><span class="params">(convolutional_layer *l)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *swap = l-&gt;weights;</span><br><span class="line">    l-&gt;weights = l-&gt;binary_weights;</span><br><span class="line">    l-&gt;binary_weights = swap;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    swap = l-&gt;weights_gpu;</span><br><span class="line">    l-&gt;weights_gpu = l-&gt;binary_weights_gpu;</span><br><span class="line">    l-&gt;binary_weights_gpu = swap;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数的作用很明显了，就要把以前的权重值替换二值化后的</p>
<p>接着<code>binarize_cpu</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">binarize_cpu</span><span class="params">(<span class="keyword">float</span> *input, <span class="keyword">int</span> n, <span class="keyword">float</span> *binary)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">        binary[i] = (input[i] &gt; <span class="number">0</span>) ? <span class="number">1</span> : <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数的第一个参数指向输入图像内存空间的指针，函数第二个参数表示一个batch的图像元素个数，函数第三个参数指向分配给二值化<code>input</code>内存空间 的指针。</p>
<p>函数很简单，总体来看函数的作用就是出入图像的二值化。</p>
<p>最后将得到的二值化输入图像赋值给原来的输入图像。</p>
<p>我们接着回到<code>forward_convolutional_layer</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//forward_convolutional_layer</span></span><br><span class="line"><span class="keyword">int</span> m = l.n/l.groups;<span class="comment">//一个group的卷积核个数</span></span><br><span class="line">   <span class="keyword">int</span> k = l.size*l.size*l.c/l.groups;<span class="comment">//一个group的卷积核元素个数</span></span><br><span class="line">   <span class="keyword">int</span> n = l.out_w*l.out_h;<span class="comment">//一个输出图像的元素个数</span></span><br><span class="line">   <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</span><br><span class="line">       <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; l.groups; ++j)&#123;</span><br><span class="line">           <span class="keyword">float</span> *a = l.weights + j*l.nweights/l.groups;</span><br><span class="line">           <span class="keyword">float</span> *b = net.workspace;</span><br><span class="line">           <span class="keyword">float</span> *c = l.output + (i*l.groups + j)*n*m;</span><br><span class="line"></span><br><span class="line">           im2col_cpu(net.input + (i*l.groups + j)*l.c/l.groups*l.h*l.w,</span><br><span class="line">               l.c/l.groups, l.h, l.w, l.size, l.stride, l.pad, b);</span><br><span class="line">           gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>这里有两个非常重要的函数<code>im2col_cpu</code>和<code>gemm</code>。先看第一个</p>
<h2 id="0x0102-im2col-cpu-amp-amp-gemm"><a href="#0x0102-im2col-cpu-amp-amp-gemm" class="headerlink" title="0x0102 im2col_cpu &amp;&amp; gemm"></a>0x0102 im2col_cpu &amp;&amp; gemm</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">im2col_get_pixel</span><span class="params">(<span class="keyword">float</span> *im, <span class="keyword">int</span> height, <span class="keyword">int</span> width, <span class="keyword">int</span> channels,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">int</span> channel, <span class="keyword">int</span> pad)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    row -= pad;</span><br><span class="line">    col -= pad;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; <span class="number">0</span> || col &lt; <span class="number">0</span> ||</span><br><span class="line">        row &gt;= height || col &gt;= width) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> im[col + width*(row + height*channel)];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//From Berkeley Vision's Caffe!</span></span><br><span class="line"><span class="comment">//https://github.com/BVLC/caffe/blob/master/LICENSE</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">im2col_cpu</span><span class="params">(<span class="keyword">float</span>* data_im,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> channels,  <span class="keyword">int</span> height,  <span class="keyword">int</span> width,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> ksize,  <span class="keyword">int</span> stride, <span class="keyword">int</span> pad, <span class="keyword">float</span>* data_col)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> c,h,w;</span><br><span class="line">    <span class="keyword">int</span> height_col = (height + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;<span class="comment">//卷积后的高度</span></span><br><span class="line">    <span class="keyword">int</span> width_col = (width + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;<span class="comment">//卷积后的宽度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> channels_col = channels * ksize * ksize;</span><br><span class="line">    <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; channels_col; ++c) &#123;</span><br><span class="line">        <span class="keyword">int</span> w_offset = c % ksize;</span><br><span class="line">        <span class="keyword">int</span> h_offset = (c / ksize) % ksize;</span><br><span class="line">        <span class="keyword">int</span> c_im = c / ksize / ksize;</span><br><span class="line">        <span class="keyword">for</span> (h = <span class="number">0</span>; h &lt; height_col; ++h) &#123;</span><br><span class="line">            <span class="keyword">for</span> (w = <span class="number">0</span>; w &lt; width_col; ++w) &#123;</span><br><span class="line">                <span class="keyword">int</span> im_row = h_offset + h * stride;</span><br><span class="line">                <span class="keyword">int</span> im_col = w_offset + w * stride;</span><br><span class="line">                <span class="keyword">int</span> col_index = (c * height_col + h) * width_col + w;</span><br><span class="line">                data_col[col_index] = im2col_get_pixel(data_im, height, width, channels,</span><br><span class="line">                        im_row, im_col, c_im, pad);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数是参考了早期caffe中的设计，但是现在caffe好像有了新的做法。首先说说这个函数的参数</p>
<ul>
<li><code>data_im</code>:指向输入数据的指针</li>
<li><code>channels</code>:一个卷积组的通道数</li>
<li><code>height</code>:输入图像的高</li>
<li><code>width</code>:输入图像的宽</li>
<li><code>ksize</code>:卷积核的大小</li>
<li><code>stride</code>:步长大小</li>
<li><code>pad</code>:pad大小</li>
<li><code>data_col</code>:指向数据转化后的内存空间</li>
</ul>
<p>这个函数比较复杂，还是举个例子说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">我们假设输入图片大小3x3，pad=1，stride=2，卷积核大小3x3，channels=1</span><br><span class="line">0 0 0 0 0</span><br><span class="line">0 1 2 3 0</span><br><span class="line">0 4 5 6 0</span><br><span class="line">0 7 8 9 0</span><br><span class="line">0 0 0 0 0</span><br><span class="line">height_col = (3+2-3)/2+1 = 2</span><br><span class="line">width_col = (3+2-3)/2+1 = 2</span><br><span class="line">channels = 1*3*3 = 9</span><br><span class="line">进入第一个循环c = 0</span><br><span class="line">w_offset = 0</span><br><span class="line">h_offset = 0</span><br><span class="line">c_im = 0</span><br><span class="line"></span><br><span class="line">h=0    w=0</span><br><span class="line">im_row = 0</span><br><span class="line">im_col = 0</span><br><span class="line">col_index = 0</span><br><span class="line">data_col[0] = 0</span><br><span class="line"></span><br><span class="line">h=0    w=1</span><br><span class="line">im_row = 0</span><br><span class="line">im_col = 1</span><br><span class="line">col_index = 1</span><br><span class="line">data_col[1] = 0</span><br><span class="line">...</span><br><span class="line">data_col[2]=0 data_col[3]=5 </span><br><span class="line">data_col[4]=0 data_col[5]=0 data_col[6]=4 data_col[7]=6</span><br><span class="line">...</span><br><span class="line">              </span><br><span class="line">0 0 0 0 0     </span><br><span class="line">0 1 2 3 0     </span><br><span class="line">0 4 5 6 0 ==&gt;  0 0 0 5 0 0 4 6 0 0 5 0 0 2 0 8 1 3 7 9 2 0 8 0 0 5 0 0 4 6 0 0 5 0 0 0</span><br><span class="line">0 7 8 9 0     </span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></table></figure>
<p>翻译成人能看得懂的就是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0 0 0 5</span><br><span class="line">0 0 4 6</span><br><span class="line">0 0 5 0</span><br><span class="line">0 2 0 8</span><br><span class="line">1 3 7 9</span><br><span class="line">2 0 8 0</span><br><span class="line">0 5 0 0</span><br><span class="line">4 6 0 0</span><br><span class="line">5 0 0 0</span><br></pre></td></tr></table></figure>
<p>这个矩阵有什么特殊的含义呢?</p>
<p>我们不难发现，这个矩阵的每一列就表示卷积核对应的一个小窗口，例如第一个窗口<code>0 0 0 0 1 2 0 4 5</code>，很有意思是不是？</p>
<p>接着我们再来看看这个<code>gemm</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gemm</span><span class="params">(<span class="keyword">int</span> TA, <span class="keyword">int</span> TB, <span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> K, <span class="keyword">float</span> ALPHA, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *A, <span class="keyword">int</span> lda, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *B, <span class="keyword">int</span> ldb,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> BETA,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *C, <span class="keyword">int</span> ldc)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    gemm_cpu( TA,  TB,  M, N, K, ALPHA,A,lda, B, ldb,BETA,C,ldc);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gemm_cpu</span><span class="params">(<span class="keyword">int</span> TA, <span class="keyword">int</span> TB, <span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> K, <span class="keyword">float</span> ALPHA, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *A, <span class="keyword">int</span> lda, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *B, <span class="keyword">int</span> ldb,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> BETA,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *C, <span class="keyword">int</span> ldc)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//printf("cpu: %d %d %d %d %d %f %d %d %f %d\n",TA, TB, M, N, K, ALPHA, lda, ldb, BETA, ldc);</span></span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; M; ++i)&#123;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; N; ++j)&#123;</span><br><span class="line">            C[i*ldc + j] *= BETA;<span class="comment">//因为前面的BETA是1，所以这里我们也不关心了</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!TA &amp;&amp; !TB)</span><br><span class="line">        gemm_nn(M, N, K, ALPHA,A,lda, B, ldb,C,ldc);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(TA &amp;&amp; !TB)</span><br><span class="line">        gemm_tn(M, N, K, ALPHA,A,lda, B, ldb,C,ldc);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(!TA &amp;&amp; TB)</span><br><span class="line">        gemm_nt(M, N, K, ALPHA,A,lda, B, ldb,C,ldc);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        gemm_tt(M, N, K, ALPHA,A,lda, B, ldb,C,ldc);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gemm_nn</span><span class="params">(<span class="keyword">int</span> M, <span class="keyword">int</span> N, <span class="keyword">int</span> K, <span class="keyword">float</span> ALPHA, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *A, <span class="keyword">int</span> lda, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *B, <span class="keyword">int</span> ldb,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">float</span> *C, <span class="keyword">int</span> ldc)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j,k;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; M; ++i)&#123;</span><br><span class="line">        <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; K; ++k)&#123;</span><br><span class="line">            <span class="keyword">register</span> <span class="keyword">float</span> A_PART = ALPHA*A[i*lda+k];</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; N; ++j)&#123;</span><br><span class="line">                C[i*ldc+j] += A_PART*B[k*ldb+j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于<code>gemm</code>前面传入的参数是<code>0,0</code>，所以我这里只看<code>gemm_nn</code>这个函数，其他函数操作相似，不再赘述。</p>
<p>我们还是先看看这个函数的参数</p>
<ul>
<li><code>M</code>: A的行数</li>
<li><code>N</code>: B的列数</li>
<li><code>K</code>: A的列数</li>
<li><code>ALPHA</code>:系数</li>
<li><code>A</code>:指向矩阵a的指针</li>
<li><code>lda</code>: a的列数</li>
<li><code>B</code>:指向矩阵b的指针</li>
<li><code>ldb</code>: b的列数</li>
<li><code>C</code>:指向矩阵c的指针</li>
<li><code>ldc</code>: c的列数</li>
</ul>
<p>我们知道这里<code>A</code>就是输入<code>weight</code>的矩阵，<code>B</code>就是我们前面<code>im2col_cpu</code>中得到的输出矩阵，<code>C</code>用来存储我们最后得到的矩阵（其实是一个数组，前面说的矩阵也是）。<code>M</code> 一个group的卷积核个数，<code>K</code>一个group的卷积核元素个数，<code>N</code> 一个输出图像的元素个数，<code>lda</code>一个group的卷积核元素个数，<code>ldb</code>一个输出图像的元素个数，<code>ldc</code>一个输出图像的元素个数。</p>
<p>我们还是举个例子说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">这里我们假设卷积核还是3x3</span><br><span class="line">权重矩阵A为</span><br><span class="line">1 2 3</span><br><span class="line">4 5 6  ==&gt; 1 2 3 4 5 6 7 8 9（应该这样写）</span><br><span class="line">7 8 9</span><br><span class="line"></span><br><span class="line">B为</span><br><span class="line">0 0 0 5</span><br><span class="line">0 0 4 6</span><br><span class="line">0 0 5 0</span><br><span class="line">0 2 0 8</span><br><span class="line">1 3 7 9</span><br><span class="line">2 0 8 0</span><br><span class="line">0 5 0 0</span><br><span class="line">4 6 0 0</span><br><span class="line">5 0 0 0</span><br><span class="line"></span><br><span class="line">C初始化后为</span><br><span class="line">1 1 1 1</span><br><span class="line"></span><br><span class="line">M=1 K=9 N=4 lda=9 ldb=4 ldb=4</span><br><span class="line">C[0]=ALPHA*A[0]*B[0]+ALPHA*A[1]*B[4]+...+ALPHA*A[8]*B[32]=95</span><br><span class="line">C[1]=107</span><br><span class="line">C[2]=107</span><br><span class="line">C[3]=95</span><br></pre></td></tr></table></figure>
<p>换成人能看懂的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">					       B</span><br><span class="line">					   [0 0 0 5</span><br><span class="line">						0 0 4 6</span><br><span class="line">						0 0 5 0</span><br><span class="line">         A				0 2 0 8       C               C</span><br><span class="line">[1 2 3 4 5 6 7 8 9]  *  1 3 7 9 + [1 1 1 1]==&gt; [95 107 107 95]</span><br><span class="line">						2 0 8 0</span><br><span class="line">						0 5 0 0</span><br><span class="line">                        4 6 0 0</span><br><span class="line">                        5 0 0 0]</span><br></pre></td></tr></table></figure>
<p>所以这两个函数的意图很明显了，就是将卷积变换成了矩阵运算。一些有意思的数学技巧^_^!!!</p>
<p>最后简要的提一下<code>gemm_nn</code> 、<code>gemm_tn</code>、<code>gemm_tt</code>、<code>gemm_nt</code>他们之间的区别，他们的命名都是有意义的。这里的<code>n</code>指的是<code>not transpose</code>而<code>t</code>指的是<code>transpose</code>。例如<code>nn</code>就表示<code>AB</code>都不转置。</p>
<p>接着我们回到<code>forward_convolutional_layer</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//forward_convolutional_layer</span></span><br><span class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        forward_batchnorm_layer(l, net);</span><br></pre></td></tr></table></figure>
<p>这里有出现一个有用的函数<code>forward_batchnorm_layer</code></p>
<h2 id="0x0103-forward-batchnorm-layer"><a href="#0x0103-forward-batchnorm-layer" class="headerlink" title="0x0103 forward_batchnorm_layer"></a>0x0103 forward_batchnorm_layer</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_batchnorm_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, net.input, <span class="number">1</span>, l.output, <span class="number">1</span>);</span><br><span class="line">    copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>上来就是一个函数<code>copy_cpu</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">copy_cpu</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">float</span> *X, <span class="keyword">int</span> INCX, <span class="keyword">float</span> *Y, <span class="keyword">int</span> INCY)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; ++i) Y[i*INCY] = X[i*INCX];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们先看一下输入的参数分别表示的是什么意思。如果我们定义了<code>BATCHNORM</code>，那么这里的<code>N</code>表示一个<code>batch</code>中的输出参数个数，<code>x</code>表示指向输入参数的指针，<code>y</code>表示指向输出参数的指针。那函数的目的很简单，将<code>net</code>中的输入，复制到<code>layer</code>中的输出；如果没有定义<code>BATCHNORM</code>，那么将<code>layer</code>中的输出复制到<code>layer</code>中的<code>x</code>。接着看后面（可以参考这篇论文<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//forward_batchnorm_layer</span></span><br><span class="line">    <span class="keyword">if</span>(net.train)&#123;</span><br><span class="line">        mean_cpu(l.output, l.batch, l.out_c, l.out_h*l.out_w, l.mean);</span><br><span class="line">        variance_cpu(l.output, l.mean, l.batch, l.out_c, l.out_h*l.out_w, l.variance);</span><br><span class="line"></span><br><span class="line">        scal_cpu(l.out_c, <span class="number">.99</span>, l.rolling_mean, <span class="number">1</span>);</span><br><span class="line">        axpy_cpu(l.out_c, <span class="number">.01</span>, l.mean, <span class="number">1</span>, l.rolling_mean, <span class="number">1</span>);</span><br><span class="line">        scal_cpu(l.out_c, <span class="number">.99</span>, l.rolling_variance, <span class="number">1</span>);</span><br><span class="line">        axpy_cpu(l.out_c, <span class="number">.01</span>, l.variance, <span class="number">1</span>, l.rolling_variance, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        normalize_cpu(l.output, l.mean, l.variance, l.batch, l.out_c, l.out_h*l.out_w);   </span><br><span class="line">        copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x_norm, <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    &#125;</span><br><span class="line">    scale_bias(l.output, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    add_bias(l.output, l.biases, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我准备把这几个函数放在一块解析，因为这几个函数都不大。先看<code>mean_cpu</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mean_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial, <span class="keyword">float</span> *mean)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> scale = <span class="number">1.</span>/(batch * spatial);<span class="comment">//求分母</span></span><br><span class="line">    <span class="keyword">int</span> i,j,k;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; filters; ++i)&#123;</span><br><span class="line">        mean[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; batch; ++j)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; spatial; ++k)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = j*filters*spatial + i*spatial + k;</span><br><span class="line">                mean[i] += x[index];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mean[i] *= scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>:指向<code>layer</code>的输出</li>
<li><code>batch</code>:一个batch的大小</li>
<li><code>filters</code>:输出的图像通道数，在这里同样可以理解为卷积核个数</li>
<li><code>spatial</code>:输出图片的大小</li>
<li><code>mean</code>:指向保存结果的指针</li>
</ul>
<p>还是举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x [95 107 107 95 1 2 3 4]</span><br><span class="line">batch = 1</span><br><span class="line">filters = 2</span><br><span class="line">spatial = 2x2 = 4</span><br><span class="line"></span><br><span class="line">scale = 1/(1x4) = 0.25</span><br><span class="line">第一次循环</span><br><span class="line">i=0 j=0</span><br><span class="line">mean[0]=0</span><br><span class="line">k=0</span><br><span class="line">index=0</span><br><span class="line">mean[0]=0+x[0]=95</span><br><span class="line">...</span><br><span class="line">mean[0]=101 mean[1]=2.5</span><br></pre></td></tr></table></figure>
<p>那么这个函数的意义就很明晰了。它要求出的是不同通道下所有输入图像的均值。对应<strong>BN</strong>论文中的这个公式</p>
<ul>
<li>$\frac{1}{m}\sum_{i=1}^m{x_i} $      //mini-batch  mean</li>
</ul>
<p>接着看<code>variance_cpu</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">variance_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial, <span class="keyword">float</span> *variance)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> scale = <span class="number">1.</span>/(batch * spatial - <span class="number">1</span>);<span class="comment">//注意这里的减1操作</span></span><br><span class="line">    <span class="keyword">int</span> i,j,k;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; filters; ++i)&#123;</span><br><span class="line">        variance[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; batch; ++j)&#123;</span><br><span class="line">            <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; spatial; ++k)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = j*filters*spatial + i*spatial + k;</span><br><span class="line">                variance[i] += <span class="built_in">pow</span>((x[index] - mean[i]), <span class="number">2</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        variance[i] *= scale;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>:指向<code>layer</code>的输出指针</li>
<li><code>mean</code>:指向上面函数输出的均值</li>
<li><code>batch</code>:<code>batch</code>大小</li>
<li><code>filters</code>:输出的图像通道数，在这里同样可以理解为卷积核个数</li>
<li><code>spatial</code>:输出图片的大小</li>
<li><code>variance</code>:指向保存结果的指针</li>
</ul>
<p>举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x [95 107 107 95 1 2 3 4]</span><br><span class="line">mean [101 25]</span><br><span class="line">batch = 1</span><br><span class="line">filters = 2</span><br><span class="line">spatial = 2x2 = 4  </span><br><span class="line">scale = 1/(1x4 - 1)=0.333</span><br><span class="line">i=0</span><br><span class="line">variance[0]=0</span><br><span class="line">j=0 k=0</span><br><span class="line">index=0</span><br><span class="line">variance[0] = 0+(95-101)^2</span><br><span class="line">...</span><br><span class="line">variance[0]=48 variance[1]=1.66666675</span><br></pre></td></tr></table></figure>
<p>那么这个函数的意义就很明晰了。它要求出的是不同通道下所有输入图像的<strong>样本方差</strong>（对于n个数据，如果n-1个确定了，那么剩下的那个就确定了（前提知道均值，均值*n - (n-1)数））。对应<strong>BN</strong>论文中的这个公式</p>
<ul>
<li>$\frac{1}{m}\sum_{i=1}^m(x_i - \mu_\beta) $       //mini-batch variance</li>
</ul>
<p>接着看<code>scal_cpu</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scal_cpu</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">float</span> ALPHA, <span class="keyword">float</span> *X, <span class="keyword">int</span> INCX)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; ++i) X[i*INCX] *= ALPHA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数很简单，就是将输入的数据乘以一个系数。</p>
<p>接着看<code>axpy_cpu</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">axpy_cpu</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">float</span> ALPHA, <span class="keyword">float</span> *X, <span class="keyword">int</span> INCX, <span class="keyword">float</span> *Y, <span class="keyword">int</span> INCY)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; ++i) Y[i*INCY] += ALPHA*X[i*INCX];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数也很简单，就是<code>Y =ALPHA*X + Y</code></p>
<p>接着看<code>normalize_cpu</code>这个函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> b, f, i;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; spatial; ++i)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = b*filters*spatial + f*spatial + i;</span><br><span class="line">                x[index] = (x[index] - mean[f])/(<span class="built_in">sqrt</span>(variance[f]) + <span class="number">.000001</span>f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>x</code>：<code>layer</code>的输出图像</li>
<li><code>mean</code>:前面算的均值</li>
<li><code>variance</code>:前面算的样本方差</li>
<li><code>batch</code>:<code>batch</code>大小</li>
<li><code>filters</code>:输出的图像通道数，在这里同样可以理解为卷积核个数</li>
<li><code>spatial</code>:输出图片的大小</li>
</ul>
<p>还是举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x [95 107 107 95 1 2 3 4]</span><br><span class="line">mean [101 25]</span><br><span class="line">variance [48 1.66666675]</span><br><span class="line">batch=1</span><br><span class="line">filters=2</span><br><span class="line">spatial = 2x2 = 4</span><br><span class="line">进入第一层循环</span><br><span class="line">b=0 f=0 i=0</span><br><span class="line">index = 0</span><br><span class="line">x[0] = (x[0]-m[0])/(sqrt(variance[0]) + 0.000001f) = -1.44</span><br><span class="line">...</span><br><span class="line">x[0]=-0.866025329 x[0]=0.866025329  x[0]=0.866025329 x[0]=-0.866025329</span><br><span class="line">x[0]=-1.16189408  x[0]=-0.387298018 x[0]=0.387298018 x[0]=1.16189408</span><br></pre></td></tr></table></figure>
<p>这个函数的作用就是一个归一化处理。对应<strong>BN</strong>论文中的这个公式</p>
<ul>
<li>$\frac{x_i-\mu_\beta}{\sqrt{\sigma_\beta^2 + \epsilon}}$       //normalize</li>
</ul>
<p>接着看<code>scale_bias</code>和<code>add_bias</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">scale_bias</span><span class="params">(<span class="keyword">float</span> *output, <span class="keyword">float</span> *scales, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j,b;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; size; ++j)&#123;</span><br><span class="line">                output[(b*n + i)*size + j] *= scales[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_bias</span><span class="params">(<span class="keyword">float</span> *output, <span class="keyword">float</span> *biases, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j,b;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; size; ++j)&#123;</span><br><span class="line">                output[(b*n + i)*size + j] += biases[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个函数的意义都很简单。对应<strong>BN</strong>论文中的这个公式</p>
<ul>
<li>$\gamma \hat{x_i}+\beta$</li>
</ul>
<p>接着我们回到<code>forward_convolutional_layer</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//forward_convolutional_layer</span></span><br><span class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        forward_batchnorm_layer(l, net);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        add_bias(l.output, l.biases, l.batch, l.n, l.out_h*l.out_w);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    activate_array(l.output, l.outputs*l.batch, l.activation);</span><br><span class="line">    <span class="keyword">if</span>(l.binary || l.xnor) swap_binary(&amp;l);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果没有设置<code>batch_normalize</code>，直接添加偏向就完事了。接着是<code>activate_array</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">activate_array</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> ACTIVATION a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">        x[i] = activate(x[i], a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">activate</span><span class="params">(<span class="keyword">float</span> x, ACTIVATION a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(a)&#123;</span><br><span class="line">        <span class="keyword">case</span> LINEAR:</span><br><span class="line">            <span class="keyword">return</span> linear_activate(x);</span><br><span class="line">        <span class="keyword">case</span> LOGISTIC:</span><br><span class="line">            <span class="keyword">return</span> logistic_activate(x);</span><br><span class="line">        <span class="keyword">case</span> LOGGY:</span><br><span class="line">            <span class="keyword">return</span> loggy_activate(x);</span><br><span class="line">        <span class="keyword">case</span> RELU:</span><br><span class="line">            <span class="keyword">return</span> relu_activate(x);</span><br><span class="line">        <span class="keyword">case</span> ELU:</span><br><span class="line">            <span class="keyword">return</span> elu_activate(x);</span><br><span class="line">        <span class="keyword">case</span> RELIE:</span><br><span class="line">            <span class="keyword">return</span> relie_activate(x);</span><br><span class="line">        <span class="keyword">case</span> RAMP:</span><br><span class="line">            <span class="keyword">return</span> ramp_activate(x);</span><br><span class="line">        <span class="keyword">case</span> LEAKY:</span><br><span class="line">            <span class="keyword">return</span> leaky_activate(x);</span><br><span class="line">        <span class="keyword">case</span> TANH:</span><br><span class="line">            <span class="keyword">return</span> tanh_activate(x);</span><br><span class="line">        <span class="keyword">case</span> PLSE:</span><br><span class="line">            <span class="keyword">return</span> plse_activate(x);</span><br><span class="line">        <span class="keyword">case</span> STAIR:</span><br><span class="line">            <span class="keyword">return</span> stair_activate(x);</span><br><span class="line">        <span class="keyword">case</span> HARDTAN:</span><br><span class="line">            <span class="keyword">return</span> hardtan_activate(x);</span><br><span class="line">        <span class="keyword">case</span> LHTAN:</span><br><span class="line">            <span class="keyword">return</span> lhtan_activate(x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">linear_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">logistic_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1.</span> + <span class="built_in">exp</span>(-x));&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">loggy_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> <span class="number">2.</span>/(<span class="number">1.</span> + <span class="built_in">exp</span>(-x)) - <span class="number">1</span>;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">relu_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> x*(x&gt;<span class="number">0</span>);&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">elu_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x &gt;= <span class="number">0</span>)*x + (x &lt; <span class="number">0</span>)*(<span class="built_in">exp</span>(x)<span class="number">-1</span>);&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">relie_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>) ? x : <span class="number">.01</span>*x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">ramp_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> x*(x&gt;<span class="number">0</span>)+<span class="number">.1</span>*x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">leaky_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (x&gt;<span class="number">0</span>) ? x : <span class="number">.1</span>*x;&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">tanh_activate</span><span class="params">(<span class="keyword">float</span> x)</span></span>&#123;<span class="keyword">return</span> (<span class="built_in">exp</span>(<span class="number">2</span>*x)<span class="number">-1</span>)/(<span class="built_in">exp</span>(<span class="number">2</span>*x)+<span class="number">1</span>);&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的意义也很明显，就是将<code>layer</code>的输出图像，输入到我们设置的激活函数中。至此<code>forward_convolutional_layer</code>中的问题全部解决。</p>
<p>好的，这篇文章的篇幅有些长了，我们把剩余部分放到下一篇</p>
<p>文章全部<a href="http://blog.csdn.net/column/details/18380.html" target="_blank" rel="noopener">YOLOv2源码分析</a></p>
<p>由于本人水平有限，文中有不对之处，希望大家指出，谢谢^_^!</p>
<p>下一篇开始分析<code>backward_convolutional_layer</code>，敬请关注。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c/">c</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/darknet/">darknet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-17-YOLOv2代码分析（二）"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/17/2017-12-17-YOLOv2代码分析（二）/">YOLOv2代码分析（二）</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/17/2017-12-17-YOLOv2代码分析（二）/" class="article-date">
	  <time datetime="2017-12-16T16:00:00.000Z" itemprop="datePublished">十二月 17, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0x01-parse-network-cfg"><a href="#0x01-parse-network-cfg" class="headerlink" title="0x01 parse_network_cfg"></a>0x01 parse_network_cfg</h1><p>我们继续前面没有说完的<code>parse_network_cfg</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg</span></span><br><span class="line">node *n = sections-&gt;front;</span><br><span class="line">   <span class="keyword">if</span>(!n) error(<span class="string">"Config file has no sections"</span>);</span><br></pre></td></tr></table></figure>
<p>我么先要了解一下<code>list</code>结构</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">list</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> size;</span><br><span class="line">    node *front;</span><br><span class="line">    node *back;</span><br><span class="line">&#125; <span class="built_in">list</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">node</span>&#123;</span></span><br><span class="line">    <span class="keyword">void</span> *val;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">node</span> *<span class="title">next</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">node</span> *<span class="title">prev</span>;</span></span><br><span class="line">&#125; node;</span><br></pre></td></tr></table></figure>
<p>这其实是一个双向链表，前向和后项都是一个<code>node</code>数据结构。这里，如果，这个链表后没有节点的话，就报错。</p>
<p>接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg</span></span><br><span class="line">network *net = make_network(sections-&gt;size - <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>这里使用了一个<code>make_network</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">network *<span class="title">make_network</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    network *net = <span class="built_in">calloc</span>(<span class="number">1</span>, <span class="keyword">sizeof</span>(network));</span><br><span class="line">    net-&gt;n = n;</span><br><span class="line">    net-&gt;layers = <span class="built_in">calloc</span>(net-&gt;n, <span class="keyword">sizeof</span>(layer));</span><br><span class="line">    net-&gt;seen = <span class="built_in">calloc</span>(<span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">size_t</span>));</span><br><span class="line">    net-&gt;t    = <span class="built_in">calloc</span>(<span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">    net-&gt;cost = <span class="built_in">calloc</span>(<span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="keyword">return</span> net;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里的这个<code>make_network</code>可能和早期的不太一样。我们先看看这里他做了什么。先看看<code>network</code>这个结构</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这个文件现在放在了darknet.h文件中</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span> &#123;</span><br><span class="line">    CONSTANT, STEP, EXP, POLY, STEPS, SIG, RANDOM</span><br><span class="line">&#125; learning_rate_policy;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">network</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> n;				<span class="comment">//网络总层数</span></span><br><span class="line">    <span class="keyword">int</span> batch;			<span class="comment">//一个batch包含的图片数目，看下面的subdivisions</span></span><br><span class="line">    <span class="keyword">size_t</span> *seen;		<span class="comment">//已经读取的图片数量</span></span><br><span class="line">    <span class="keyword">int</span> *t;</span><br><span class="line">    <span class="keyword">float</span> epoch;		<span class="comment">//训练的次数</span></span><br><span class="line">    <span class="keyword">int</span> subdivisions;	<span class="comment">//注意前面的batch/subdivisions才是网络的batch大小，可能目的是防止gpu显存不够</span></span><br><span class="line">    layer *layers;		<span class="comment">//指向网络的层</span></span><br><span class="line">    <span class="keyword">float</span> *output;</span><br><span class="line">    learning_rate_policy policy;<span class="comment">//学习率的策略，是一个枚举类型</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> learning_rate;<span class="comment">//学习率</span></span><br><span class="line">    <span class="keyword">float</span> momentum;		<span class="comment">//动量，一般0.9</span></span><br><span class="line">    <span class="keyword">float</span> decay;		<span class="comment">//权重衰减正则项，防止过拟合</span></span><br><span class="line">    <span class="keyword">float</span> gamma;		<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">float</span> scale;		<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">float</span> power;		<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">int</span> time_steps;</span><br><span class="line">    <span class="keyword">int</span> step;			<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">int</span> max_batches;	<span class="comment">//最大的训练batch数目</span></span><br><span class="line">    <span class="keyword">float</span> *scales;		<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">int</span>   *steps;		<span class="comment">//用于计算学习率，见后面0x0102</span></span><br><span class="line">    <span class="keyword">int</span> num_steps;		<span class="comment">//steps中的数据个数</span></span><br><span class="line">    <span class="keyword">int</span> burn_in;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> adam;			<span class="comment">//adam算法</span></span><br><span class="line">    <span class="keyword">float</span> B1;			<span class="comment">//一阶矩估计的指数衰减率</span></span><br><span class="line">    <span class="keyword">float</span> B2;			<span class="comment">//二阶矩估计的指数衰减率</span></span><br><span class="line">    <span class="keyword">float</span> eps;			<span class="comment">//为了防止在实现中除以零</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> inputs;			<span class="comment">//h*w*c</span></span><br><span class="line">    <span class="keyword">int</span> outputs;</span><br><span class="line">    <span class="keyword">int</span> truths;</span><br><span class="line">    <span class="keyword">int</span> notruth;</span><br><span class="line">    <span class="keyword">int</span> h, w, c;			<span class="comment">//输入图像的高，宽，通道数</span></span><br><span class="line">    <span class="keyword">int</span> max_crop;			<span class="comment">//控制图片缩放的最大值</span></span><br><span class="line">    <span class="keyword">int</span> min_crop;			<span class="comment">//控制图片缩放的最小值</span></span><br><span class="line">    <span class="keyword">float</span> max_ratio;		<span class="comment">//控制图片缩放的最大比例</span></span><br><span class="line">    <span class="keyword">float</span> min_ratio;		<span class="comment">//控制图片缩放的最小比例</span></span><br><span class="line">    <span class="keyword">int</span> center;</span><br><span class="line">    <span class="keyword">float</span> angle;			<span class="comment">//设置旋转角度，扩充数据</span></span><br><span class="line">    <span class="keyword">float</span> aspect;			<span class="comment">//设置方位，扩充数据</span></span><br><span class="line">    <span class="keyword">float</span> exposure;			<span class="comment">//设置饱和度，扩充数据</span></span><br><span class="line">    <span class="keyword">float</span> saturation;		<span class="comment">//设置曝光量，扩充数据</span></span><br><span class="line">    <span class="keyword">float</span> hue;				<span class="comment">//设置色调，扩充数据</span></span><br><span class="line">    <span class="keyword">int</span> random;				<span class="comment">//random为1时随机使用不同尺寸的图片进行训练</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> gpu_index;			<span class="comment">//设置第几个gpu</span></span><br><span class="line">    tree *hierarchy;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> *input;</span><br><span class="line">    <span class="keyword">float</span> *truth;</span><br><span class="line">    <span class="keyword">float</span> *delta;</span><br><span class="line">    <span class="keyword">float</span> *workspace;</span><br><span class="line">    <span class="keyword">int</span> train;</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="keyword">float</span> *cost;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    <span class="keyword">float</span> *input_gpu;</span><br><span class="line">    <span class="keyword">float</span> *truth_gpu;</span><br><span class="line">    <span class="keyword">float</span> *delta_gpu;</span><br><span class="line">    <span class="keyword">float</span> *output_gpu;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">&#125; network;</span><br></pre></td></tr></table></figure>
<p>由于参数太多，用到哪个说哪个，这个结构的主要作用就是存储网络的配置参数。<code>make_network</code>的作用就是产生<code>network</code>这种数据结构。接着往下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg</span></span><br><span class="line">net-&gt;gpu_index = gpu_index;<span class="comment">//设置gpu</span></span><br><span class="line">   size_params params;</span><br></pre></td></tr></table></figure>
<p>又出现一个新的结构<code>size_params</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">size_params</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> batch;		<span class="comment">//一个batch包含的图片数目</span></span><br><span class="line">    <span class="keyword">int</span> inputs;</span><br><span class="line">    <span class="keyword">int</span> h;			<span class="comment">//图像的高</span></span><br><span class="line">    <span class="keyword">int</span> w;			<span class="comment">//输入图像的宽</span></span><br><span class="line">    <span class="keyword">int</span> c;			<span class="comment">//输入图像的通道数</span></span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="keyword">int</span> time_steps;</span><br><span class="line">    network *net;</span><br><span class="line">&#125; size_params;</span><br></pre></td></tr></table></figure>
<p>接着往下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">//parse_network_cfg</span></span><br><span class="line">section *s = (section *)n-&gt;val;<span class="comment">//section这个结构我在（一）中提过</span></span><br></pre></td></tr></table></figure>
<p><code>n</code>是一个<code>node</code>结构，这个结构中的<code>val</code>是一个<code>void*</code>，所以这里就是将<code>node</code>结构中的<code>val</code>强转为<code>section*</code>，相当于我在（一）中图上画的<code>[net]</code>等节点。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> 	<span class="comment">//parse_network_cfg   </span></span><br><span class="line"><span class="built_in">list</span> *options = s-&gt;options;<span class="comment">//这里就是之前说的kvp，也就是size=3,stride=1,pad=1这些</span></span><br><span class="line">   <span class="keyword">if</span>(!is_network(s)) error(<span class="string">"First section must be [net] or [network]"</span>);</span><br></pre></td></tr></table></figure>
<p>看一下这个<code>is_network</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">is_network</span><span class="params">(section *s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">strcmp</span>(s-&gt;type, <span class="string">"[net]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(s-&gt;type, <span class="string">"[network]"</span>)==<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用很明显，判断传入的第一个<code>section</code>是不是<code>[net]</code>或<code>[network]</code>。接着又是一个比较大的函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg    </span></span><br><span class="line">parse_net_options(options, net);</span><br></pre></td></tr></table></figure>
<h2 id="0x0101-parse-net-options"><a href="#0x0101-parse-net-options" class="headerlink" title="0x0101 parse_net_options"></a>0x0101 parse_net_options</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">parse_net_options</span><span class="params">(<span class="built_in">list</span> *options, network *net)</span><span class="comment">//传入的时options参数和我们的network</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    net-&gt;batch = option_find_int(options, <span class="string">"batch"</span>,<span class="number">1</span>);<span class="comment">//设置net的batch大小</span></span><br><span class="line">    net-&gt;learning_rate = option_find_float(options, <span class="string">"learning_rate"</span>, <span class="number">.001</span>);<span class="comment">//设置学习率</span></span><br><span class="line">    net-&gt;momentum = option_find_float(options, <span class="string">"momentum"</span>, <span class="number">.9</span>);<span class="comment">//设置动量</span></span><br><span class="line">    net-&gt;decay = option_find_float(options, <span class="string">"decay"</span>, <span class="number">.0001</span>);<span class="comment">//设置权重衰减</span></span><br><span class="line">    <span class="keyword">int</span> subdivs = option_find_int(options, <span class="string">"subdivisions"</span>,<span class="number">1</span>);<span class="comment">//设置subdivisions，防止显存不够</span></span><br><span class="line">    net-&gt;time_steps = option_find_int_quiet(options, <span class="string">"time_steps"</span>,<span class="number">1</span>);</span><br><span class="line">    net-&gt;notruth = option_find_int_quiet(options, <span class="string">"notruth"</span>,<span class="number">0</span>);</span><br><span class="line">    net-&gt;batch /= subdivs;</span><br><span class="line">    net-&gt;batch *= net-&gt;time_steps;</span><br><span class="line">    net-&gt;subdivisions = subdivs;</span><br><span class="line">    net-&gt;random = option_find_int_quiet(options, <span class="string">"random"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    net-&gt;adam = option_find_int_quiet(options, <span class="string">"adam"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>(net-&gt;adam)&#123;<span class="comment">//设置adam参数，这里的默认选项是按照adam论文给的参数设置的</span></span><br><span class="line">        net-&gt;B1 = option_find_float(options, <span class="string">"B1"</span>, <span class="number">.9</span>);</span><br><span class="line">        net-&gt;B2 = option_find_float(options, <span class="string">"B2"</span>, <span class="number">.999</span>);</span><br><span class="line">        net-&gt;eps = option_find_float(options, <span class="string">"eps"</span>, <span class="number">.0000001</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    net-&gt;h = option_find_int_quiet(options, <span class="string">"height"</span>,<span class="number">0</span>);</span><br><span class="line">    net-&gt;w = option_find_int_quiet(options, <span class="string">"width"</span>,<span class="number">0</span>);</span><br><span class="line">    net-&gt;c = option_find_int_quiet(options, <span class="string">"channels"</span>,<span class="number">0</span>);</span><br><span class="line">    net-&gt;inputs = option_find_int_quiet(options, <span class="string">"inputs"</span>, net-&gt;h * net-&gt;w * net-&gt;c);</span><br><span class="line">    net-&gt;max_crop = option_find_int_quiet(options, <span class="string">"max_crop"</span>,net-&gt;w*<span class="number">2</span>);</span><br><span class="line">    net-&gt;min_crop = option_find_int_quiet(options, <span class="string">"min_crop"</span>,net-&gt;w);</span><br><span class="line">    net-&gt;max_ratio = option_find_float_quiet(options, <span class="string">"max_ratio"</span>, (<span class="keyword">float</span>) net-&gt;max_crop / net-&gt;w);</span><br><span class="line">    net-&gt;min_ratio = option_find_float_quiet(options, <span class="string">"min_ratio"</span>, (<span class="keyword">float</span>) net-&gt;min_crop / net-&gt;w);</span><br><span class="line">    net-&gt;center = option_find_int_quiet(options, <span class="string">"center"</span>,<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    net-&gt;angle = option_find_float_quiet(options, <span class="string">"angle"</span>, <span class="number">0</span>);</span><br><span class="line">    net-&gt;aspect = option_find_float_quiet(options, <span class="string">"aspect"</span>, <span class="number">1</span>);</span><br><span class="line">    net-&gt;saturation = option_find_float_quiet(options, <span class="string">"saturation"</span>, <span class="number">1</span>);</span><br><span class="line">    net-&gt;exposure = option_find_float_quiet(options, <span class="string">"exposure"</span>, <span class="number">1</span>);</span><br><span class="line">    net-&gt;hue = option_find_float_quiet(options, <span class="string">"hue"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!net-&gt;inputs &amp;&amp; !(net-&gt;h &amp;&amp; net-&gt;w &amp;&amp; net-&gt;c)) error(<span class="string">"No input parameters supplied"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *policy_s = option_find_str(options, <span class="string">"policy"</span>, <span class="string">"constant"</span>);</span><br><span class="line">    net-&gt;policy = get_policy(policy_s);</span><br><span class="line">    net-&gt;burn_in = option_find_int_quiet(options, <span class="string">"burn_in"</span>, <span class="number">0</span>);</span><br><span class="line">    net-&gt;power = option_find_float_quiet(options, <span class="string">"power"</span>, <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">if</span>(net-&gt;policy == STEP)&#123;<span class="comment">//如果学习率的策略是STEP的话</span></span><br><span class="line">        net-&gt;step = option_find_int(options, <span class="string">"step"</span>, <span class="number">1</span>);</span><br><span class="line">        net-&gt;scale = option_find_float(options, <span class="string">"scale"</span>, <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (net-&gt;policy == STEPS)&#123;<span class="comment">//如果学习率的策略是STEPS的话</span></span><br><span class="line">        <span class="keyword">char</span> *l = option_find(options, <span class="string">"steps"</span>);<span class="comment">//指向steps的字符串</span></span><br><span class="line">        <span class="keyword">char</span> *p = option_find(options, <span class="string">"scales"</span>);<span class="comment">//指向scales的字符串</span></span><br><span class="line">        <span class="keyword">if</span>(!l || !p) error(<span class="string">"STEPS policy must have steps and scales in cfg file"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> len = <span class="built_in">strlen</span>(l);</span><br><span class="line">        <span class="keyword">int</span> n = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> i;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len; ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span> (l[i] == <span class="string">','</span>) ++n;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> *steps = <span class="built_in">calloc</span>(n, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));<span class="comment">//将所有的steps值分开存放到这个数组中</span></span><br><span class="line">        <span class="keyword">float</span> *scales = <span class="built_in">calloc</span>(n, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));<span class="comment">//将所有的scales值分开存放到这个数组中</span></span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">            <span class="keyword">int</span> step    = atoi(l);</span><br><span class="line">            <span class="keyword">float</span> scale = atof(p);</span><br><span class="line">            l = <span class="built_in">strchr</span>(l, <span class="string">','</span>)+<span class="number">1</span>;</span><br><span class="line">            p = <span class="built_in">strchr</span>(p, <span class="string">','</span>)+<span class="number">1</span>;</span><br><span class="line">            steps[i] = step;</span><br><span class="line">            scales[i] = scale;</span><br><span class="line">        &#125;</span><br><span class="line">        net-&gt;scales = scales;</span><br><span class="line">        net-&gt;steps = steps;</span><br><span class="line">        net-&gt;num_steps = n;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (net-&gt;policy == EXP)&#123;</span><br><span class="line">        net-&gt;gamma = option_find_float(options, <span class="string">"gamma"</span>, <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (net-&gt;policy == SIG)&#123;</span><br><span class="line">        net-&gt;gamma = option_find_float(options, <span class="string">"gamma"</span>, <span class="number">1</span>);</span><br><span class="line">        net-&gt;step = option_find_int(options, <span class="string">"step"</span>, <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (net-&gt;policy == POLY || net-&gt;policy == RANDOM)&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    net-&gt;max_batches = option_find_int(options, <span class="string">"max_batches"</span>, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数中出现了这个函数<code>option_find_int_quiet</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">option_find_int_quiet</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key, <span class="keyword">int</span> def)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *v = option_find(l, key);</span><br><span class="line">    <span class="keyword">if</span>(v) <span class="keyword">return</span> atoi(v);</span><br><span class="line">    <span class="keyword">return</span> def;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">option_find</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    node *n = l-&gt;front;</span><br><span class="line">    <span class="keyword">while</span>(n)&#123;</span><br><span class="line">        kvp *p = (kvp *)n-&gt;val;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">strcmp</span>(p-&gt;key, key) == <span class="number">0</span>)&#123;</span><br><span class="line">            p-&gt;used = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">return</span> p-&gt;val;</span><br><span class="line">        &#125;</span><br><span class="line">        n = n-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数和之前的<code>option_find_int</code>不同。首先看里面的<code>option_find</code>这个函数的作用就是查找<code>list</code>中，<code>node</code>的<code>key</code>和参数的<code>key</code>相同的<code>node</code>，返回这个<code>node</code>的<code>val</code>，如果不存在，返回0。通过这个函数我们得到了<code>pad=1 stride=2</code>等参数后的数值信息。接下来就很easy了，如果有这个参数就将这个字符串（得到的是一个字符串，不是一个数）转化为一个整数，没有的话就返回三个参数（有点类似于默认参数）。</p>
<p>回顾整个<code>option_find_int_quiet</code>，它的作用就是找出和<code>key</code>的<code>node</code>的数值大小。类似于一种<code>map</code>里面的查找操作。</p>
<p>而再看<code>option_find_int</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">option_find_int</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key, <span class="keyword">int</span> def)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *v = option_find(l, key);</span><br><span class="line">    <span class="keyword">if</span>(v) <span class="keyword">return</span> atoi(v);</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s: Using default '%d'\n"</span>, key, def);</span><br><span class="line">    <span class="keyword">return</span> def;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它和前者的区别在于，它会打印报错信息。</p>
<h2 id="0x0102-学习率策略"><a href="#0x0102-学习率策略" class="headerlink" title="0x0102 学习率策略"></a>0x0102 学习率策略</h2><p>学习率策略的设置是一个枚举类型</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span> &#123;</span><br><span class="line">    CONSTANT, STEP, EXP, POLY, STEPS, SIG, RANDOM</span><br><span class="line">&#125; learning_rate_policy;</span><br></pre></td></tr></table></figure>
<p>这是在前面就提到的。我们现在来看看，这几个有什么区别（参考caffe源码）</p>
<ul>
<li><code>CONSTANT</code>:学习率是一个固定的值learning_rate</li>
<li><code>STEP</code>:是一种均匀分步策略learning_rate* gamma ^ (floor(iter / step))</li>
<li><code>EXP</code>:learning_rate* gamma ^ iter</li>
<li><code>POLY</code>:learning_rate(1 - iter/max_iter) ^ (power)</li>
<li><code>STEPS</code>:同STEP只是这里的scale和step是一个数组</li>
<li><code>SIG</code>:learning_rate ( 1/(1 + exp(-gamma * (iter - stepsize))))</li>
<li><code>RANDOM</code>:代码中没有考虑</li>
</ul>
<p>回头看整个<code>parse_net_options</code>这个函数，这个函数的主要功能就是读取<code>[net]</code>后的信息，赋值到<code>net</code>所指向的<code>network</code>结构中。</p>
<p>接着我们再回到<code>parse_network_cfg</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg </span></span><br><span class="line">    params.h = net-&gt;h;			<span class="comment">//将h，w，c赋值size_params对象params，下面类似不再赘述</span></span><br><span class="line">    params.w = net-&gt;w;</span><br><span class="line">    params.c = net-&gt;c;</span><br><span class="line">    params.inputs = net-&gt;inputs;</span><br><span class="line">    params.batch = net-&gt;batch;</span><br><span class="line">    params.time_steps = net-&gt;time_steps;</span><br><span class="line">    params.net = net;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">size_t</span> workspace_size = <span class="number">0</span>;</span><br><span class="line">    n = n-&gt;next;				<span class="comment">//[net]搞定了，接下来去下一个node</span></span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    free_section(s);</span><br></pre></td></tr></table></figure>
<p>我们再来看看这个<code>free_section</code>函数做了什么</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">free_section</span><span class="params">(section *s)</span><span class="comment">//传入的变量是之前的那个section指针</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">free</span>(s-&gt;type);<span class="comment">//释放type空间</span></span><br><span class="line">    node *n = s-&gt;options-&gt;front;<span class="comment">//以下内容是释放s指向的kvp链表</span></span><br><span class="line">    <span class="keyword">while</span>(n)&#123;</span><br><span class="line">        kvp *pair = (kvp *)n-&gt;val;</span><br><span class="line">        <span class="built_in">free</span>(pair-&gt;key);</span><br><span class="line">        <span class="built_in">free</span>(pair);</span><br><span class="line">        node *next = n-&gt;next;</span><br><span class="line">        <span class="built_in">free</span>(n);</span><br><span class="line">        n = next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">free</span>(s-&gt;options);</span><br><span class="line">    <span class="built_in">free</span>(s);<span class="comment">//最后释放s</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>综上来看这个函数的目的在这里很明显了。我们把<code>cfg</code>的参数从<code>section</code>中copy到了<code>network</code>中，<code>section</code>内存不用了，自然要把它释放。</p>
<p>再回到<code>parse_network_cfg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//parse_network_cfg </span></span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"layer     filters    size              input                output\n"</span>);</span><br><span class="line">   <span class="keyword">while</span>(n)&#123;</span><br><span class="line">       params.index = count;</span><br><span class="line">       <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%5d "</span>, count);</span><br><span class="line">       s = (section *)n-&gt;val;</span><br><span class="line">       options = s-&gt;options;</span><br><span class="line">       layer l = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">       LAYER_TYPE lt = string_to_layer_type(s-&gt;type);</span><br></pre></td></tr></table></figure>
<p>这是一个非常大的循环体，先看前面一小部分。我们先看看其中的<code>LAYER_TYPE</code>结构和<code>layer</code>结构</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//现在这个结构也放在了darknet.h中</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">layer</span>;</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">layer</span> <span class="title">layer</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span> &#123;</span><br><span class="line">    CONVOLUTIONAL,</span><br><span class="line">    DECONVOLUTIONAL,</span><br><span class="line">    CONNECTED,</span><br><span class="line">    MAXPOOL,</span><br><span class="line">    SOFTMAX,</span><br><span class="line">    DETECTION,</span><br><span class="line">    DROPOUT,</span><br><span class="line">    CROP,</span><br><span class="line">    ROUTE,</span><br><span class="line">    COST,</span><br><span class="line">    NORMALIZATION,</span><br><span class="line">    AVGPOOL,</span><br><span class="line">    LOCAL,</span><br><span class="line">    SHORTCUT,</span><br><span class="line">    ACTIVE,</span><br><span class="line">    RNN,</span><br><span class="line">    GRU,</span><br><span class="line">    LSTM,</span><br><span class="line">    CRNN,</span><br><span class="line">    BATCHNORM,</span><br><span class="line">    NETWORK,</span><br><span class="line">    XNOR,</span><br><span class="line">    REGION,</span><br><span class="line">    REORG,</span><br><span class="line">    BLANK</span><br><span class="line">&#125; LAYER_TYPE;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">layer</span>&#123;</span></span><br><span class="line">    LAYER_TYPE type;</span><br><span class="line">    ACTIVATION activation;</span><br><span class="line">    COST_TYPE cost_type;</span><br><span class="line">    <span class="keyword">void</span> (*forward)   (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*backward)  (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*update)    (struct layer, update_args);</span><br><span class="line">    <span class="keyword">void</span> (*forward_gpu)   (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*backward_gpu)  (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*update_gpu)    (struct layer, update_args);</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>
<p>我们可以看到<code>LAYER_TYPE</code>就是每一层的类型。而<code>layer</code>就是设置这些层的参数，由于参数太多，我在后面会分开讲。</p>
<p>再来看<code>string_to_layer_type</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">LAYER_TYPE <span class="title">string_to_layer_type</span><span class="params">(<span class="keyword">char</span> * type)</span><span class="comment">//传入的参数就是之前的section-&gt;type</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[shortcut]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> SHORTCUT;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[crop]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> CROP;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[cost]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> COST;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[detection]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> DETECTION;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[region]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> REGION;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[local]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LOCAL;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[conv]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[convolutional]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> CONVOLUTIONAL;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[deconv]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[deconvolutional]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> DECONVOLUTIONAL;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[activation]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> ACTIVE;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[net]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[network]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> NETWORK;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[crnn]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> CRNN;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[gru]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> GRU;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[lstm]"</span>) == <span class="number">0</span>) <span class="keyword">return</span> LSTM;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[rnn]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> RNN;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[conn]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[connected]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> CONNECTED;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[max]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[maxpool]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> MAXPOOL;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[reorg]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> REORG;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[avg]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[avgpool]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> AVGPOOL;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[dropout]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> DROPOUT;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[lrn]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[normalization]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> NORMALIZATION;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[batchnorm]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> BATCHNORM;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[soft]"</span>)==<span class="number">0</span></span><br><span class="line">            || <span class="built_in">strcmp</span>(type, <span class="string">"[softmax]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> SOFTMAX;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(type, <span class="string">"[route]"</span>)==<span class="number">0</span>) <span class="keyword">return</span> ROUTE;</span><br><span class="line">    <span class="keyword">return</span> BLANK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用很明显，通过比较字符串，将原先的<code>section</code>中的<code>type</code>变成了<code>LAYER_TYPE</code>中的枚举元素。</p>
<p>接着回到<code>parse_network_cfg</code>，后面就是很多的条件判断</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">		<span class="comment">//parse_network_cfg </span></span><br><span class="line">		<span class="keyword">if</span>(lt == CONVOLUTIONAL)&#123;</span><br><span class="line">            l = parse_convolutional(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == DECONVOLUTIONAL)&#123;</span><br><span class="line">            l = parse_deconvolutional(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == LOCAL)&#123;</span><br><span class="line">            l = parse_local(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == ACTIVE)&#123;</span><br><span class="line">            l = parse_activation(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == RNN)&#123;</span><br><span class="line">            l = parse_rnn(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == GRU)&#123;</span><br><span class="line">            l = parse_gru(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (lt == LSTM) &#123;</span><br><span class="line">            l = parse_lstm(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == CRNN)&#123;</span><br><span class="line">            l = parse_crnn(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == CONNECTED)&#123;</span><br><span class="line">            l = parse_connected(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == CROP)&#123;</span><br><span class="line">            l = parse_crop(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == COST)&#123;</span><br><span class="line">            l = parse_cost(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == REGION)&#123;</span><br><span class="line">            l = parse_region(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == DETECTION)&#123;</span><br><span class="line">            l = parse_detection(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == SOFTMAX)&#123;</span><br><span class="line">            l = parse_softmax(options, params);</span><br><span class="line">            net-&gt;hierarchy = l.softmax_tree;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == NORMALIZATION)&#123;</span><br><span class="line">            l = parse_normalization(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == BATCHNORM)&#123;</span><br><span class="line">            l = parse_batchnorm(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == MAXPOOL)&#123;</span><br><span class="line">            l = parse_maxpool(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == REORG)&#123;</span><br><span class="line">            l = parse_reorg(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == AVGPOOL)&#123;</span><br><span class="line">            l = parse_avgpool(options, params);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == ROUTE)&#123;</span><br><span class="line">            l = parse_route(options, params, net);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == SHORTCUT)&#123;</span><br><span class="line">            l = parse_shortcut(options, params, net);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(lt == DROPOUT)&#123;</span><br><span class="line">            l = parse_dropout(options, params);</span><br><span class="line">            l.output = net-&gt;layers[count<span class="number">-1</span>].output;</span><br><span class="line">            l.delta = net-&gt;layers[count<span class="number">-1</span>].delta;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">            l.output_gpu = net-&gt;layers[count<span class="number">-1</span>].output_gpu;</span><br><span class="line">            l.delta_gpu = net-&gt;layers[count<span class="number">-1</span>].delta_gpu;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>然后再看以<code>parse</code>开头的函数作用，以其中一个为例<code>parse_convolutional</code></p>
<h2 id="0x0103-parse-convolutional"><a href="#0x0103-parse-convolutional" class="headerlink" title="0x0103 parse_convolutional"></a>0x0103 parse_convolutional</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">convolutional_layer <span class="title">parse_convolutional</span><span class="params">(<span class="built_in">list</span> *options, size_params params)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = option_find_int(options, <span class="string">"filters"</span>,<span class="number">1</span>);	<span class="comment">//卷积核个数</span></span><br><span class="line">    <span class="keyword">int</span> size = option_find_int(options, <span class="string">"size"</span>,<span class="number">1</span>);	<span class="comment">//卷积核大小</span></span><br><span class="line">    <span class="keyword">int</span> stride = option_find_int(options, <span class="string">"stride"</span>,<span class="number">1</span>);<span class="comment">//步长</span></span><br><span class="line">    <span class="keyword">int</span> pad = option_find_int_quiet(options, <span class="string">"pad"</span>,<span class="number">0</span>);<span class="comment">//图像周围是否补0</span></span><br><span class="line">    <span class="keyword">int</span> padding = option_find_int_quiet(options, <span class="string">"padding"</span>,<span class="number">0</span>);<span class="comment">//补0的长度</span></span><br><span class="line">    <span class="keyword">int</span> groups = option_find_int_quiet(options, <span class="string">"groups"</span>, <span class="number">1</span>);<span class="comment">//卷积核组的个数</span></span><br><span class="line">    <span class="keyword">if</span>(pad) padding = size/<span class="number">2</span>;<span class="comment">//对应SAME补0策略</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *activation_s = option_find_str(options, <span class="string">"activation"</span>, <span class="string">"logistic"</span>);<span class="comment">//激活函数</span></span><br><span class="line">    ACTIVATION activation = get_activation(activation_s);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> batch,h,w,c;</span><br><span class="line">    h = params.h;		<span class="comment">//图片的高</span></span><br><span class="line">    w = params.w;		<span class="comment">//图片的宽</span></span><br><span class="line">    c = params.c;		<span class="comment">//图片的通道数</span></span><br><span class="line">    batch=params.batch;</span><br><span class="line">    <span class="keyword">if</span>(!(h &amp;&amp; w &amp;&amp; c)) error(<span class="string">"Layer before convolutional layer must output image."</span>);</span><br><span class="line">    <span class="keyword">int</span> batch_normalize = option_find_int_quiet(options, <span class="string">"batch_normalize"</span>, <span class="number">0</span>);<span class="comment">//BN操作</span></span><br><span class="line">    <span class="keyword">int</span> binary = option_find_int_quiet(options, <span class="string">"binary"</span>, <span class="number">0</span>);<span class="comment">//权重二值化</span></span><br><span class="line">    <span class="keyword">int</span> xnor = option_find_int_quiet(options, <span class="string">"xnor"</span>, <span class="number">0</span>);<span class="comment">//权重和输入二值化</span></span><br><span class="line"></span><br><span class="line">    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net-&gt;adam);</span><br><span class="line">    layer.flipped = option_find_int_quiet(options, <span class="string">"flipped"</span>, <span class="number">0</span>);</span><br><span class="line">    layer.dot = option_find_float_quiet(options, <span class="string">"dot"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为我之前已经讲过了<code>option_find_int</code>函数，所以这里不再多说了。代码的前面部分也非常容易理解，就射设置，不同<code>[]</code>后面的参数。</p>
<p>后面又出现一个新的结构<code>ACTIVATION</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span>&#123;</span><br><span class="line">    LOGISTIC, RELU, RELIE, LINEAR, RAMP, TANH, PLSE, LEAKY, ELU, LOGGY, STAIR, HARDTAN, LHTAN</span><br><span class="line">&#125; ACTIVATION;</span><br></pre></td></tr></table></figure>
<p>很明显这个枚举是用来定义不同的激活函数的。</p>
<p>再看<code>get_activation</code>这个函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ACTIVATION <span class="title">get_activation</span><span class="params">(<span class="keyword">char</span> *s)</span><span class="comment">//传入的参数时cfg中的activation</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"logistic"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LOGISTIC;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"loggy"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LOGGY;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"relu"</span>)==<span class="number">0</span>) <span class="keyword">return</span> RELU;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"elu"</span>)==<span class="number">0</span>) <span class="keyword">return</span> ELU;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"relie"</span>)==<span class="number">0</span>) <span class="keyword">return</span> RELIE;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"plse"</span>)==<span class="number">0</span>) <span class="keyword">return</span> PLSE;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"hardtan"</span>)==<span class="number">0</span>) <span class="keyword">return</span> HARDTAN;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"lhtan"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LHTAN;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"linear"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LINEAR;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"ramp"</span>)==<span class="number">0</span>) <span class="keyword">return</span> RAMP;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"leaky"</span>)==<span class="number">0</span>) <span class="keyword">return</span> LEAKY;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"tanh"</span>)==<span class="number">0</span>) <span class="keyword">return</span> TANH;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(s, <span class="string">"stair"</span>)==<span class="number">0</span>) <span class="keyword">return</span> STAIR;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Couldn't find activation function %s, going with ReLU\n"</span>, s);</span><br><span class="line">    <span class="keyword">return</span> RELU;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用也很明显，通过比较字符串，将原先的激活函数的字符串，转化为现在的<code>ACTIVATION</code>枚举元素。</p>
<p>接着看<code>convolutional_layer</code>这个结构</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> layer convolutional_layer;</span><br></pre></td></tr></table></figure>
<p>其实就是一个<code>layer</code>，再看<code>make_convolutional_layer</code>函数（又是一个非常大的函数）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">convolutional_layer <span class="title">make_convolutional_layer</span><span class="params">(<span class="keyword">int</span> batch, <span class="keyword">int</span> h, <span class="keyword">int</span> w, <span class="keyword">int</span> c, <span class="keyword">int</span> n, <span class="keyword">int</span> groups, <span class="keyword">int</span> size, <span class="keyword">int</span> stride, <span class="keyword">int</span> padding, ACTIVATION activation, <span class="keyword">int</span> batch_normalize, <span class="keyword">int</span> binary, <span class="keyword">int</span> xnor, <span class="keyword">int</span> adam)</span><span class="comment">//传入的参数就是我们之前设置好的</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    convolutional_layer l = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    l.type = CONVOLUTIONAL;</span><br><span class="line"></span><br><span class="line">    l.groups = groups;	<span class="comment">//卷积核的组数</span></span><br><span class="line">    l.h = h;			<span class="comment">//图像的高</span></span><br><span class="line">    l.w = w;			<span class="comment">//图像的宽</span></span><br><span class="line">    l.c = c;			<span class="comment">//图像的通道数目</span></span><br><span class="line">    l.n = n;			<span class="comment">//卷积核个数</span></span><br><span class="line">    l.binary = binary;</span><br><span class="line">    l.xnor = xnor;</span><br><span class="line">    l.batch = batch;</span><br><span class="line">    l.stride = stride;</span><br><span class="line">    l.size = size;</span><br><span class="line">    l.pad = padding;</span><br><span class="line">    l.batch_normalize = batch_normalize;</span><br><span class="line">	</span><br><span class="line">    l.weights = <span class="built_in">calloc</span>(c/groups*n*size*size, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));<span class="comment">//计算所有权重个数，c/groups*n*size*size，分配内存空间</span></span><br><span class="line">    l.weight_updates = <span class="built_in">calloc</span>(c/groups*n*size*size, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    l.biases = <span class="built_in">calloc</span>(n, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));<span class="comment">//卷积核个数和偏向的数目一致，分配内存空间</span></span><br><span class="line">    l.bias_updates = <span class="built_in">calloc</span>(n, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    l.nweights = c/groups*n*size*size;</span><br><span class="line">    l.nbiases = n;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// float scale = 1./sqrt(size*size*c);</span></span><br><span class="line">    <span class="keyword">float</span> scale = <span class="built_in">sqrt</span>(<span class="number">2.</span>/(size*size*c/l.groups));<span class="comment">//缩放系数</span></span><br><span class="line">    <span class="comment">//scale = .02;</span></span><br><span class="line">    <span class="comment">//for(i = 0; i &lt; c*n*size*size; ++i) l.weights[i] = scale*rand_uniform(-1, 1);</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.nweights; ++i) l.weights[i] = scale*rand_normal();<span class="comment">//初始化权重</span></span><br><span class="line">    <span class="keyword">int</span> out_w = convolutional_out_width(l);</span><br><span class="line">    <span class="keyword">int</span> out_h = convolutional_out_height(l);</span><br></pre></td></tr></table></figure>
<p>这里出现了新的函数，我们只分析其中一个<code>convolutional_out_width</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">convolutional_out_height</span><span class="params">(convolutional_layer l)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (l.h + <span class="number">2</span>*l.pad - l.size) / l.stride + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">convolutional_out_width</span><span class="params">(convolutional_layer l)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (l.w + <span class="number">2</span>*l.pad - l.size) / l.stride + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数中这个公式大家应该很熟悉，就是计算卷积后的输出图像的大小。</p>
<p>接着看<code>make_convolutional_layer</code>这个函数后面的部分</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//make_convolutional_layer</span></span><br><span class="line">	l.out_h = out_h;	<span class="comment">//输出图像的高</span></span><br><span class="line">    l.out_w = out_w;	<span class="comment">//输出图像的宽</span></span><br><span class="line">    l.out_c = n;		<span class="comment">//输出图像的通道数</span></span><br><span class="line">    l.outputs = l.out_h * l.out_w * l.out_c;<span class="comment">//输出图像的总元素个数</span></span><br><span class="line">    l.inputs = l.w * l.h * l.c;				<span class="comment">//输入图像的总元素个数</span></span><br><span class="line"></span><br><span class="line">    l.output = <span class="built_in">calloc</span>(l.batch*l.outputs, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    l.delta  = <span class="built_in">calloc</span>(l.batch*l.outputs, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br></pre></td></tr></table></figure>
<p>好的，这篇文章的篇幅有些长了，我们把剩余部分放到下一篇</p>
<p>文章全部<a href="http://blog.csdn.net/column/details/18380.html" target="_blank" rel="noopener">YOLOv2源码分析</a></p>
<p>由于本人水平有限，文中有不对之处，希望大家指出，谢谢^_^!</p>
<p>下一篇继续分析<code>make_convolutional_layer</code>这个函数后面的部分，敬请关注。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c/">c</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/darknet/">darknet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-16-YOLOv2代码分析（一）"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/16/2017-12-16-YOLOv2代码分析（一）/">YOLOv2代码分析（一）</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/16/2017-12-16-YOLOv2代码分析（一）/" class="article-date">
	  <time datetime="2017-12-15T16:00:00.000Z" itemprop="datePublished">十二月 16, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="0x00-写在开头"><a href="#0x00-写在开头" class="headerlink" title="0x00 写在开头"></a>0x00 写在开头</h1><p>写这一系列文章主要是想解析yolov2的具体实现，因为在作者的论文中有很多地方没有进行详细表述，所以不看源代码的话很难知道幕后具体做了什么。另一点是学习一下别人的思路，因为你要知道作者的代码相当于自己写了一个小型框架（函数的接口设计的可能不是非常好）。</p>
<h1 id="0x01-从main函数开始"><a href="#0x01-从main函数开始" class="headerlink" title="0x01 从main函数开始"></a>0x01 从main函数开始</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//test_resize("data/bad.jpg");</span></span><br><span class="line">    <span class="comment">//test_box();</span></span><br><span class="line">    <span class="comment">//test_convolutional_layer();</span></span><br><span class="line">    <span class="keyword">if</span>(argc &lt; <span class="number">2</span>)&#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s &lt;function&gt;\n"</span>, argv[<span class="number">0</span>]);<span class="comment">//如果参数小于2就打印出错信息</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;<span class="comment">//出错后返回</span></span><br><span class="line">    &#125;</span><br><span class="line">    gpu_index = find_int_arg(argc, argv, <span class="string">"-i"</span>, <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>接着看到<code>find_int_arg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_int_arg</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv, <span class="keyword">char</span> *arg, <span class="keyword">int</span> def)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; argc<span class="number">-1</span>; ++i)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!argv[i]) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[i], arg))&#123;</span><br><span class="line">            def = atoi(argv[i+<span class="number">1</span>]);</span><br><span class="line">            del_arg(argc, argv, i);</span><br><span class="line">            del_arg(argc, argv, i);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> def;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>find_int_arg</code>这个函数本身的目的是要找出参数中的<code>int</code>值。在这里主要任务就是判断输入参数是不是有<code>-i</code>，将<code>-i</code>后一位的数值转化为<code>int</code>，然后返回这个值。其中又出现了两次<code>del_arg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">del_arg</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = index; i &lt; argc<span class="number">-1</span>; ++i) argv[i] = argv[i+<span class="number">1</span>];</span><br><span class="line">    argv[i] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数作用是删除<code>index</code>位置的参数。此处调用两次的作用是将<code>-i</code>和其后的数值去除，类似于一个列表前移操作，后面的项补0。</p>
<p>接着看main函数后面的</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(find_arg(argc, argv, <span class="string">"-nogpu"</span>)) &#123;</span><br><span class="line">        gpu_index = <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里调用了一个<code>find_arg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_arg</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[], <span class="keyword">char</span> *arg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; argc; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span>(!argv[i]) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[i], arg)) &#123;</span><br><span class="line">            del_arg(argc, argv, i);</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用就是查看参数中是否有<code>arg</code>指向的字符串。在这里如果参数中出现了<code>-nogpu</code>则我们<code>gpu_index</code>设置为<code>-1</code>，也就是不使用<code>gpu</code></p>
<p>接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> GPU</span></span><br><span class="line">    gpu_index = <span class="number">-1</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="keyword">if</span>(gpu_index &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        cuda_set_device(gpu_index);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>如果没有定义GPU这个宏，那么将 <code>gpu_index</code>设置为 <code>-1</code>。如果设置了，并且我们前面也没有关闭<code>gpu</code>选项的话，那么调用<code>cuda_set_device</code>这个函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cuda_set_device</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    gpu_index = n;</span><br><span class="line">    cudaError_t status = cudaSetDevice(n);<span class="comment">//这是cuda编程里面的，不详细说。设置显卡编号</span></span><br><span class="line">    check_error(status);<span class="comment">//判断返回信息，设置显卡成功了，还是失败了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">0</span> == <span class="built_in">strcmp</span>(argv[<span class="number">1</span>], <span class="string">"yolo"</span>))&#123;</span><br><span class="line">        run_yolo(argc, argv);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里有很多选项，我先看我最感兴趣的<code>yolo</code>选项</p>
<p>到这里main函数中的所有问题就理清楚了，接着就是<code>run_yolo</code>函数中问题了</p>
<h1 id="0x02-run-yolo"><a href="#0x02-run-yolo" class="headerlink" title="0x02 run_yolo"></a>0x02 run_yolo</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run_yolo</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *prefix = find_char_arg(argc, argv, <span class="string">"-prefix"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">float</span> thresh = find_float_arg(argc, argv, <span class="string">"-thresh"</span>, <span class="number">.2</span>);</span><br><span class="line">    <span class="keyword">int</span> cam_index = find_int_arg(argc, argv, <span class="string">"-c"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">int</span> frame_skip = find_int_arg(argc, argv, <span class="string">"-s"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>(argc &lt; <span class="number">4</span>)&#123;<span class="comment">//如果参数小于4，打印出错信息</span></span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n"</span>, argv[<span class="number">0</span>], argv[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> avg = find_int_arg(argc, argv, <span class="string">"-avg"</span>, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">char</span> *cfg = argv[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">char</span> *weights = (argc &gt; <span class="number">4</span>) ? argv[<span class="number">4</span>] : <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *filename = (argc &gt; <span class="number">5</span>) ? argv[<span class="number">5</span>]: <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//根据第三个参数选择调用的函数</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"test"</span>)) test_yolo(cfg, weights, filename, thresh);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"train"</span>)) train_yolo(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"valid"</span>)) validate_yolo(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"recall"</span>)) validate_yolo_recall(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"demo"</span>)) demo(cfg, weights, thresh, cam_index, filename, voc_names, <span class="number">20</span>, frame_skip, prefix, avg, <span class="number">.5</span>, <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有<code>find_char_arg</code>，<code>find_float_arg</code>函数，这里就不再赘述了，按照前面解析<code>find_int_arg</code>的思路去做。</p>
<p>首先<code>cfg</code>这个指针指向<code>cfg</code>文件名字符串，<code>weight</code>指向了权重文件名字符串。别的变量暂时不管，因为我们先关注<code>train_yolo</code>这个函数。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">train_yolo</span><span class="params">(<span class="keyword">char</span> *cfgfile, <span class="keyword">char</span> *weightfile)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *train_images = <span class="string">"/data/voc/train.txt"</span>;<span class="comment">//train_images指向train.txt路径字符串</span></span><br><span class="line">    <span class="keyword">char</span> *backup_directory = <span class="string">"/home/pjreddie/backup/"</span>;<span class="comment">//backup_directory指向保存权重文件的路径</span></span><br><span class="line">    srand(time(<span class="number">0</span>));<span class="comment">//设置随机数种子</span></span><br><span class="line">    <span class="keyword">char</span> *base = basecfg(cfgfile);<span class="comment">//cfgfile就是上面说的cfg指向的字符串</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s\n"</span>, base);</span><br></pre></td></tr></table></figure>
<p>好的，这里出现了一个<code>basecfg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">basecfg</span><span class="params">(<span class="keyword">char</span> *cfgfile)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *c = cfgfile;</span><br><span class="line">    <span class="keyword">char</span> *next;</span><br><span class="line">    <span class="keyword">while</span>((next = <span class="built_in">strchr</span>(c, <span class="string">'/'</span>)))</span><br><span class="line">    &#123;</span><br><span class="line">        c = next+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    c = copy_string(c);</span><br><span class="line">    next = <span class="built_in">strchr</span>(c, <span class="string">'.'</span>);</span><br><span class="line">    <span class="keyword">if</span> (next) *next = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">copy_string</span><span class="params">(<span class="keyword">char</span> *s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *copy = <span class="built_in">malloc</span>(<span class="built_in">strlen</span>(s)+<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">strncpy</span>(copy, s, <span class="built_in">strlen</span>(s)+<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> copy;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先看传入的参数<code>cfgfile</code>，是一个<code>cfg</code>文件的路径字符串。接着<code>strchr</code>，这个函数的作用是去第一个参数中，第二个参数以后的字符包括第二个参数（abc/ab.cfg—&gt;/ab.cfg），接着<code>c=next+1</code>，也就是<code>c</code>指向了这个<code>cfg</code>文件名字符串。</p>
<p><code>copy_string</code>函数的作用，就是重新分配一块内存，并且内容保留。那这里<code>next</code>后的操作就很清楚了，就是把<code>.cfg</code>后缀去掉。</p>
<p>这个函数是有缺陷的，因为这里没有考虑到window用户的需求，应该增加<code>\\</code>的处理。</p>
<p>接着回到<code>train_yolo</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//train_yolo	</span></span><br><span class="line"><span class="keyword">float</span> avg_loss = <span class="number">-1</span>;</span><br><span class="line">   network net = parse_network_cfg(cfgfile);</span><br></pre></td></tr></table></figure>
<p>这里出现了<code>parse_network_cfg</code>函数</p>
<h1 id="0x03-parse-network-cfg"><a href="#0x03-parse-network-cfg" class="headerlink" title="0x03 parse_network_cfg"></a>0x03 parse_network_cfg</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">network *<span class="title">parse_network_cfg</span><span class="params">(<span class="keyword">char</span> *filename)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">list</span> *sections = read_cfg(filename);</span><br></pre></td></tr></table></figure>
<p>出现了一个<code>read_cfg</code>函数</p>
<h2 id="0x0301-read-cfg"><a href="#0x0301-read-cfg" class="headerlink" title="0x0301 read_cfg"></a>0x0301 read_cfg</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">list</span> *<span class="title">read_cfg</span><span class="params">(<span class="keyword">char</span> *filename)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE *file = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span>(file == <span class="number">0</span>) file_error(filename);</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">file_error</span><span class="params">(<span class="keyword">char</span> *s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Couldn't open file: %s\n"</span>, s);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>file_error</code>判断<code>cfg</code>文件有没有打开失败。接着往后</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read_cfg</span></span><br><span class="line">    <span class="keyword">char</span> *line;</span><br><span class="line">    <span class="keyword">int</span> nu = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">list</span> *options = make_list();<span class="comment">//创建一个链表</span></span><br><span class="line">    section *current = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>((line=fgetl(file)) != <span class="number">0</span>)&#123;</span><br></pre></td></tr></table></figure>
<p>这里出现了一个<code>fgetl</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">fgetl</span><span class="params">(FILE *fp)</span><span class="comment">//fp指向打开后的cfg文件</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(feof(fp)) <span class="keyword">return</span> <span class="number">0</span>;<span class="comment">//如果文件结尾，退出</span></span><br><span class="line">    <span class="keyword">size_t</span> size = <span class="number">512</span>;</span><br><span class="line">    <span class="keyword">char</span> *line = <span class="built_in">malloc</span>(size*<span class="keyword">sizeof</span>(<span class="keyword">char</span>));<span class="comment">//分配512字节内存</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">//从fp中读取一行数据到line中，数据最大为size。</span></span><br><span class="line">    <span class="comment">//注意，如果碰到换行或文件eof会停止读入。读取失败返回NULL</span></span><br><span class="line">    <span class="keyword">if</span>(!fgets(line, size, fp))&#123;</span><br><span class="line">        <span class="built_in">free</span>(line);<span class="comment">//失败就释放内存</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">size_t</span> curr = <span class="built_in">strlen</span>(line);<span class="comment">//返回line的长度，也就是读入的字符个数</span></span><br><span class="line">	</span><br><span class="line">    <span class="comment">//这里的代码是为了处理size不够的情况</span></span><br><span class="line">    <span class="keyword">while</span>((line[curr<span class="number">-1</span>] != <span class="string">'\n'</span>) &amp;&amp; !feof(fp))&#123;</span><br><span class="line">        <span class="keyword">if</span>(curr == size<span class="number">-1</span>)&#123;</span><br><span class="line">            <span class="comment">//size不够我们就变大两倍</span></span><br><span class="line">            size *= <span class="number">2</span>;</span><br><span class="line">            line = <span class="built_in">realloc</span>(line, size*<span class="keyword">sizeof</span>(<span class="keyword">char</span>));</span><br><span class="line">            <span class="keyword">if</span>(!line) &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%ld\n"</span>, size);</span><br><span class="line">                malloc_error();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//line不够，也就是一行没有读全，那么不会再从开始，而是接着上一次没有读完的信息</span></span><br><span class="line">        <span class="keyword">size_t</span> readsize = size-curr;</span><br><span class="line">        <span class="keyword">if</span>(readsize &gt; INT_MAX) readsize = INT_MAX<span class="number">-1</span>;</span><br><span class="line">        fgets(&amp;line[curr], readsize, fp);</span><br><span class="line">        curr = <span class="built_in">strlen</span>(line);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(line[curr<span class="number">-1</span>] == <span class="string">'\n'</span>) line[curr<span class="number">-1</span>] = <span class="string">'\0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> line;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用，简单理解就是读取文件的一行。其实用c++中的<code>getline</code>函数就可以解决了。同样的python中的<code>readline</code>也可以做到。</p>
<p>接着回到<code>read_cfg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read_cfg</span></span><br><span class="line">		++ nu;</span><br><span class="line">        strip(line);</span><br></pre></td></tr></table></figure>
<p>出现一个<code>strip</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">strip</span><span class="params">(<span class="keyword">char</span> *s)</span><span class="comment">//传入我们前面读入的行</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> i;</span><br><span class="line">    <span class="keyword">size_t</span> len = <span class="built_in">strlen</span>(s);</span><br><span class="line">    <span class="keyword">size_t</span> offset = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//这里的做法和list前移一样，出现空格符，则其后的所有项前移</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len; ++i)&#123;</span><br><span class="line">        <span class="keyword">char</span> c = s[i];</span><br><span class="line">        <span class="keyword">if</span>(c==<span class="string">' '</span>||c==<span class="string">'\t'</span>||c==<span class="string">'\n'</span>) ++offset;</span><br><span class="line">        <span class="keyword">else</span> s[i-offset] = c;</span><br><span class="line">    &#125;</span><br><span class="line">    s[len-offset] = <span class="string">'\0'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数的作用就是删除字符串中的空格符（’\n’,’\t’,’ ‘）</p>
<p>回到<code>read_cfg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">		<span class="keyword">switch</span>(line[<span class="number">0</span>])&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'['</span>:<span class="comment">//这里就是看读入的行的第一个字符是'['也就是对于cfg文件中[net],[maxpool]这种东西</span></span><br><span class="line">                current = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(section));<span class="comment">//创建一个current</span></span><br><span class="line">                list_insert(options, current);<span class="comment">//将current插入之前建立的options链表</span></span><br><span class="line">                current-&gt;options = make_list();<span class="comment">//给current创建链表</span></span><br><span class="line">                current-&gt;type = line;<span class="comment">//将读入的[net],[maxpool]读入type</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'\0'</span>:</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'#'</span>:</span><br><span class="line">            <span class="keyword">case</span> <span class="string">';'</span>:</span><br><span class="line">                <span class="built_in">free</span>(line);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">if</span>(!read_option(line, current-&gt;options))&#123;</span><br><span class="line">                    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Config file error line %d, could parse: %s\n"</span>, nu, line);</span><br><span class="line">                    <span class="built_in">free</span>(line);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fclose(file);</span><br><span class="line">    <span class="keyword">return</span> options;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先看一下<code>section</code>这个结构体的定义</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> *type;</span><br><span class="line">    <span class="built_in">list</span> *options;</span><br><span class="line">&#125;section;</span><br></pre></td></tr></table></figure>
<p>它的内部包含一个链表。这里作者的<code>list_insert(options, current);</code>中options和后面的<code>current-&gt;options = make_list();</code>中的options存在歧义。其实两者一毛钱关系都没有。</p>
<p>分析一下这个<code>read_option</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">read_option</span><span class="params">(<span class="keyword">char</span> *s, <span class="built_in">list</span> *options)</span><span class="comment">//s指向读取的行，list就是一个section中的list</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> i;</span><br><span class="line">    <span class="keyword">size_t</span> len = <span class="built_in">strlen</span>(s);</span><br><span class="line">    <span class="keyword">char</span> *val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; len; ++i)&#123;</span><br><span class="line">        <span class="keyword">if</span>(s[i] == <span class="string">'='</span>)&#123;</span><br><span class="line">            s[i] = <span class="string">'\0'</span>;</span><br><span class="line">            val = s+i+<span class="number">1</span>;<span class="comment">//val指向=后面的字符串</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i == len<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *key = s;<span class="comment">//这个时候key指向的是=前面的字符串</span></span><br><span class="line">    option_insert(options, key, val);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> *key;</span><br><span class="line">    <span class="keyword">char</span> *val;</span><br><span class="line">    <span class="keyword">int</span> used;</span><br><span class="line">&#125; kvp;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">option_insert</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key, <span class="keyword">char</span> *val)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    kvp *p = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(kvp));</span><br><span class="line">    p-&gt;key = key;</span><br><span class="line">    p-&gt;val = val;</span><br><span class="line">    p-&gt;used = <span class="number">0</span>;</span><br><span class="line">    list_insert(l, p);<span class="comment">//将一个kvp结构插入section中的list</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>回头再看这个<code>switch</code>，他在这里的作用就是将<code>cfg</code>文件中的不同内容（’[net]’,’[maxpool]’）区分开，然后存到一个列表中。</p>
<p>举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">filters=32</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">activation=leaky</span><br></pre></td></tr></table></figure>
<p>这是<code>yolo9000.cfg</code>中的一个片段，我们先看第一行，他是一个’[]’，所以进入第一个判断，我们首先将<code>[convolutional]</code>字符串，存入一个<code>section</code>对象的<code>type</code>中，并且将这个<code>section</code>对象插入到一个列表中。接着读取第二行<code>batch_normalize=1</code>，将<code>=</code>前后内容拆开存储到<code>kvp</code>结构中，再将这个<code>kvp</code>插入到<code>section</code>的<code>list</code>中。</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fmkrbpakepj20v1063jrc.jpg"><br></center>

<p>总览整个<code>read_cfg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">list</span> *<span class="title">read_cfg</span><span class="params">(<span class="keyword">char</span> *filename)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE *file = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span>(file == <span class="number">0</span>) file_error(filename);</span><br><span class="line">    <span class="keyword">char</span> *line;</span><br><span class="line">    <span class="keyword">int</span> nu = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">list</span> *options = make_list();</span><br><span class="line">    section *current = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>((line=fgetl(file)) != <span class="number">0</span>)&#123;</span><br><span class="line">        ++ nu;</span><br><span class="line">        strip(line);</span><br><span class="line">        <span class="keyword">switch</span>(line[<span class="number">0</span>])&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'['</span>:</span><br><span class="line">                current = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(section));</span><br><span class="line">                list_insert(options, current);</span><br><span class="line">                current-&gt;options = make_list();</span><br><span class="line">                current-&gt;type = line;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'\0'</span>:</span><br><span class="line">            <span class="keyword">case</span> <span class="string">'#'</span>:</span><br><span class="line">            <span class="keyword">case</span> <span class="string">';'</span>:</span><br><span class="line">                <span class="built_in">free</span>(line);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">if</span>(!read_option(line, current-&gt;options))&#123;</span><br><span class="line">                    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Config file error line %d, could parse: %s\n"</span>, nu, line);</span><br><span class="line">                    <span class="built_in">free</span>(line);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fclose(file);</span><br><span class="line">    <span class="keyword">return</span> options;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>作者做了一种数据结构来存放<code>cfg</code>的文件数据。</p>
<p>文章全部<a href="http://blog.csdn.net/column/details/18380.html" target="_blank" rel="noopener">YOLOv2源码分析</a></p>
<p>由于本人水平有限，文中有不对之处，希望大家指出，谢谢^_^!</p>
<p>下一篇继续分析<code>parse_network_cfg</code>这个函数后面的部分，敬请关注。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c/">c</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/darknet/">darknet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-15-目标检测中的IOU计算问题"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/15/2017-12-15-目标检测中的IOU计算问题/">目标检测中的IOU计算问题</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/15/2017-12-15-目标检测中的IOU计算问题/" class="article-date">
	  <time datetime="2017-12-14T16:00:00.000Z" itemprop="datePublished">十二月 15, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>在目标检测中我们有一个基本的操作，形象表述就是画框框，我们要通过我们画的框把我们要标注的目标给框出来，如下面那个狗。我们既可以说这个狗被框出来了，但是也可以说没有，因为你可以观察到左边和上面是存在一些缝隙的。</p>
<center class="half"><br><img src="http://wx4.sinaimg.cn/mw690/af2d2659ly1fmh85d281aj20lc0g0tub.jpg" width="350" height="300"><br></center>

<p>那问题就出现了。什么样的框才算把目标给框住了呢？这个时候就有了IOU这个评价指标。什么是IOU？</p>
<center class="half"><br><img src="http://wx3.sinaimg.cn/mw690/af2d2659ly1fmhhfup0kyj20br07e741.jpg" width="350" height="300"><br></center>

<p>IOU想要描述的时黄框（检测到的结果）和绿框（标注的结果）重合的程度。用公式表示为</p>
<ul>
<li>$IOU = \frac{DetectionResult\bigcap GroundTruth}{DetectionResult\bigcup GroundTruth}$</li>
</ul>
<p>公式里面的<code>DetectionResult</code>就表示黄框（也就是通过神经网络得到的结果），<code>GroundResult</code>表示绿框（也就是标注的结果）。</p>
<p>好的，这看上去很容易，但是在具体实现的时候就会有一些问题了。接下来我先用YOLO2中的做法讲解其实现原理。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">overlap</span><span class="params">(<span class="keyword">float</span> x1, <span class="keyword">float</span> w1, <span class="keyword">float</span> x2, <span class="keyword">float</span> w2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> l1 = x1 - w1/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">float</span> l2 = x2 - w2/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">float</span> left = l1 &gt; l2 ? l1 : l2;</span><br><span class="line">    <span class="keyword">float</span> r1 = x1 + w1/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">float</span> r2 = x2 + w2/<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">float</span> right = r1 &lt; r2 ? r1 : r2;</span><br><span class="line">    <span class="keyword">return</span> right - left;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">box_intersection</span><span class="params">(box a, box b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> w = overlap(a.x, a.w, b.x, b.w);</span><br><span class="line">    <span class="keyword">float</span> h = overlap(a.y, a.h, b.y, b.h);</span><br><span class="line">    <span class="keyword">if</span>(w &lt; <span class="number">0</span> || h &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> area = w*h;</span><br><span class="line">    <span class="keyword">return</span> area;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">box_union</span><span class="params">(box a, box b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> i = box_intersection(a, b);</span><br><span class="line">    <span class="keyword">float</span> u = a.w*a.h + b.w*b.h - i;</span><br><span class="line">    <span class="keyword">return</span> u;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">box_iou</span><span class="params">(box a, box b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> box_intersection(a, b)/box_union(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个代码中<code>box a</code>表示结果<code>box</code>，而<code>box b</code>表示的是标注<code>box</code>，这两者都是<code>box</code>类的对象，这个类在这里我们用到了这样几个属性：</p>
<ul>
<li><code>x</code>:表示box中心点的x坐标</li>
<li><code>y</code>:表示box中心点的y坐标</li>
<li><code>w</code>:表示box的宽度</li>
<li><code>h</code>:表示box的高度</li>
</ul>
<p>我么先看第一个函数<code>overlap</code>是干什么的。假设按照<code>box_intersection</code>中的第一个做法，传入这样几个参数<code>overlap(a.x, a.w, b.x, b.w)</code>，结果就是<code>l1</code>表示黄框的左边，<code>l2</code>表示绿框的左边，<code>r1</code>表示黄框的右边，<code>r2</code>表示绿框的右边。如下图</p>
<center class="half"><br><img src="http://wx3.sinaimg.cn/mw690/af2d2659ly1fmhi58wr3uj20ga0bijr8.jpg" width="350" height="300"><br></center>

<p>接着<code>left=l2</code>，<code>right=r1</code>，最后<code>right-left=交集的宽</code>。同理传入<code>overlap(a.y, a.h, b.y, b.h)</code></p>
<p>得到交集的高。两个一乘即为交集的大小。<code>box_iou</code>函数中的做法大家应该可以看得明白了，不再赘述。</p>
<p>当我们训练好网络后，将待测图像输入网络得到的输出结果是一个高维矩阵。在YOLO2中最后得到的矩阵是这样的<code>[-1, H, W, B, (4 + 1 + C)]</code>。</p>
<ul>
<li><code>H</code>:表示纵向分割的块的数目</li>
<li><code>W</code>:表示横向分割的块的数目</li>
</ul>
<p>也就是一幅图片被我们分割为了<code>H*W</code>块。</p>
<ul>
<li><code>B</code>:表示anchors的数目</li>
<li><code>C</code>:表示classes的数目</li>
</ul>
<p>注意这里的<code>4</code>表示前面讲的<code>x,y,w,h</code>，而后面<code>1</code>表示<code>confidence</code>（参看论文中的$Pr(object)*IOU$）</p>
<p>以下是对应的python tensorflow版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">coords = tf.reshape(coords, [<span class="number">-1</span>, H*W, B, <span class="number">4</span>])</span><br><span class="line">wh = tf.exp(coords[:,:,:,<span class="number">2</span>:<span class="number">4</span>]) * np.reshape(anchors, [<span class="number">1</span>, <span class="number">1</span>, B, <span class="number">2</span>])</span><br><span class="line">area_pred = wh[:,:,:,<span class="number">0</span>] * wh[:,:,:,<span class="number">1</span>]<span class="comment">#得到预测框的面积</span></span><br><span class="line">centers = coords[:,:,:,<span class="number">0</span>:<span class="number">2</span>]<span class="comment">#得到预测框的xy</span></span><br><span class="line">floor = centers - (wh * <span class="number">.5</span>)</span><br><span class="line">ceil  = centers + (wh * <span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the intersection areas</span></span><br><span class="line">intersect_upleft   = tf.maximum(floor, _upleft)</span><br><span class="line">intersect_botright = tf.minimum(ceil , _botright)</span><br><span class="line">intersect_wh = intersect_botright - intersect_upleft</span><br><span class="line">intersect_wh = tf.maximum(intersect_wh, <span class="number">0.0</span>)</span><br><span class="line">intersect = tf.multiply(intersect_wh[:,:,:,<span class="number">0</span>], intersect_wh[:,:,:,<span class="number">1</span>])<span class="comment">#得到交集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the best IOU, set 0.0 confidence for worse boxes</span></span><br><span class="line">iou = tf.truediv(intersect, _areas + area_pred - intersect)<span class="comment">#_areas表示真实的面积</span></span><br></pre></td></tr></table></figure>
<p>这里的<code>floor</code>对应于上面的<code>l1</code>、黄框的下边，<code>ceil</code>对应于上面的<code>r1</code>、黄框的上边。<code>_upleft</code>对应上面的<code>l2</code>和绿框的下边，<code>_botright</code>对应上面的<code>r2</code>和绿框的上边。<code>intersect_upleft</code>就是<code>l2</code>和黄框的下边，<code>intersect_botright</code>就是<code>r1</code>和绿框的上边，后面的代码就顺理成章了。</p>
<p>由于本人水平有限，如有问题，恳请指出！^_^</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IOU/">IOU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/c/">c</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-13-tf.identity的作用"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/13/2017-12-13-tf.identity的作用/">tf.identity的作用</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/13/2017-12-13-tf.identity的作用/" class="article-date">
	  <time datetime="2017-12-12T16:00:00.000Z" itemprop="datePublished">十二月 13, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>还是从一个<a href="https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for" target="_blank" rel="noopener">例子</a>开始讲起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)<span class="comment">#对x进行加1操作</span></span><br><span class="line"><span class="comment">#tf.control_dependencies的作用是:在执行y=x前，先执行x_plus_1</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = x</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>这个代码的输出和我们想象的不一样，他的输出是<code>0, 0, 0, 0, 0</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">x_plus_1 = tf.assign_add(x, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([x_plus_1]):</span><br><span class="line">    y = tf.identity(x)</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">5</span>):</span><br><span class="line">        print(y.eval())</span><br></pre></td></tr></table></figure>
<p>这段代码的输出才是<code>1, 2, 3, 4, 5</code></p>
<p>其实这里的<code>tf.identity</code>的作用相当于一个<code>reference</code>，也就是<code>y</code>仅仅是一个<code>x</code>的别名，这两个变量使用的是一块内存，而对于<code>y=x</code>，<code>x</code>与<code>y</code>在不同的内存中。</p>
<p>这里举一个经典的例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap1</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    a^=b;  </span><br><span class="line">    b^=a;  </span><br><span class="line">    a^=b;  </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap2</span><span class="params">(<span class="keyword">int</span> &amp;a,<span class="keyword">int</span> &amp;b)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    a^=b;  </span><br><span class="line">    b^=a;  </span><br><span class="line">    a^=b;  </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a=<span class="number">1</span>,b=<span class="number">2</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; b &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//12</span></span><br><span class="line">    swap1(a, b);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; b &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//12</span></span><br><span class="line">    swap2(a, b);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; b &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//21</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然这是我的理解，可能有不对之处，欢迎大家指出！</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/identity/">identity</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-06-TensorBoard图形可视化"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/06/2017-12-06-TensorBoard图形可视化/">TensorBoard计算图可视化</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/06/2017-12-06-TensorBoard图形可视化/" class="article-date">
	  <time datetime="2017-12-05T16:00:00.000Z" itemprop="datePublished">十二月 6, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>TensorFlow计算图功能强大但复杂。 图表可视化可以帮助您理解和调试它们。 这是一个可视化工作的例子。</p>
<center class="half"><br><img src="http://wx2.sinaimg.cn/large/af2d2659ly1fm4kh0mnwug21040iw7wj.gif"><br></center>


<h2 id="命名空间和节点"><a href="#命名空间和节点" class="headerlink" title="命名空间和节点"></a>命名空间和节点</h2><p>典型的TensorFlow图可能有成千上万个节点 - 太多的节点很难一次看到，甚至无法使用标准的图形工具进行布局。 为简化起见，变量名声明在作用域内，可视化使用这些信息来定义图中节点上的层次结构。 默认情况下，只显示该层次结构的顶部。 下面是一个使用<code>tf.name_scope</code>在<code>hidden</code>名称范围下定义三个操作的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  a = tf.constant(<span class="number">5</span>, name=<span class="string">'alpha'</span>)</span><br><span class="line">  W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">  b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'biases'</span>)</span><br></pre></td></tr></table></figure>
<p>这产生了以下三个操作名称：</p>
<ul>
<li><p><code>hidden/alpha</code></p>
</li>
<li><p><code>hidden/weights</code></p>
</li>
<li><p><code>hidden/biases</code></p>
</li>
</ul>
<p>默认情况下，可视化文件将全部折叠为标记为隐藏的节点。 额外的细节不会丢失。 你可以双击，或者点击右上角的橙色<code>+</code>符号来展开节点，然后你会看到三个子节点，分别是<code>alpha</code>，<code>weight</code>和<code>bias</code>。</p>
<p>这是一个复杂节点在其初始状态和扩展状态的例子。</p>
<center class="half"><br><img src="http://wx2.sinaimg.cn/mw690/af2d2659ly1fm4kgw1iifj20tr0f30u2.jpg"><br></center>

<p>按命名空间对节点进行分组对于制作清晰的图形至关重要。如果您正在构建模型，则命名空间可以控制生成的可视化图像。你的命名空间越好，你的可视化就越好。</p>
<p>上图说明了可视化的第二个方面。 TensorFlow图有两种连接：数据相关性和控制相关性。数据相关性显示两个操作符之间的张量流，并用实线箭头显示，而控制相关性使用虚线。在扩展视图（上图右侧）中，除了连接<code>CheckNumerics</code>和<code>control_dependency</code>的虚线外，所有连接都是数据依赖关系。</p>
<p>还有一个简化布局的技巧。大多数TensorFlow图有几个与其他节点连接的节点。例如，许多节点可能对初始化步骤具有控制依赖性。绘制<code>init</code>节点及其依赖关系之间的所有边将创建一个非常混乱的视图。</p>
<p>为了减少混乱，可视化将所有高度节点分隔到右侧的辅助区域，并不画线来表示其边缘。我们绘制小节点图标来代替连线。分离出的辅助节点通常不会去除关键信息，因为这些节点通常与簿记功能相关。有关如何在主图形和辅助区域之间移动节点的信息，请参阅交互。</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4kgwn70jj20ts09rmyj.jpg"><br></center>

<p>最后一个结构简化是<em>series collapsing</em>连续图案 - 也就是说，名称相差最后一个数字并具有同构结构的节点 - 会折叠成一堆节点，如下所示。 对于长序列的网络，这大大简化了视图。 与分层节点一样，双击将扩展该系列。 请参阅交互以了解如何为特定节点集禁用/启用系列折叠。</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4kgx2inuj20tm04twez.jpg"><br></center>

<p>最后，作为易读性的最后一个帮助，可视化对常量和汇总节点使用特殊的图标。 总结为下面的节点符号表：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4kgxhputj20tq0fbjsq.jpg"><br></center>


<h2 id="交互"><a href="#交互" class="headerlink" title="交互"></a>交互</h2><p>通过平移和缩放导航图形。 点击并拖动以平移，并使用滚动手势进行缩放。 双击某个节点，或单击其<code>+</code>按钮，展开一个代表一组操作的命名空间。 为了在放大和平移时轻松跟踪当前视点，右下角会有一个小地图。</p>
<p>要关闭打开的节点，请再次双击它或单击其<code>-</code>按钮。 您也可以单击一次以选择一个节点， 它会变成一个较深的颜色，并且关于它的详细信息以及它所连接的节点将出现在右上角的可视化对象信息卡中。</p>
<center class="half"><br><img src="http://wx4.sinaimg.cn/mw690/af2d2659ly1fm4kgy45psj20tr0c4mzt.jpg"><br></center>

<p>TensorBoard提供了几种方法来改变图形的视觉布局。这不会改变图的计算语义，但是它可以使网络的结构变得清晰。通过右键单击某个节点或按该节点信息卡底部的按钮，可以对其布局进行以下更改：</p>
<ul>
<li>节点可以在主图表和辅助区域之间移动。</li>
<li>可以将一系列节点取消分组，使得该系列中的节点不会出现在一起。未分组的序列也可以重新组合。</li>
</ul>
<p><code>Selection</code>也可以帮助理解高层次节点。选择任何高层次节点，其他连接的相应节点图标也将被选中。这可以很容易地看到哪些节点正在保存 - 哪些不是。</p>
<p>点击信息卡中的节点名称将选择它。如有必要，视点将自动平移，以便节点可见。</p>
<p>最后，您可以使用图例上方的颜色菜单为图形选择两种配色方案。默认的显示结构：当两个高层节点具有相同的结构时，它们以相同颜色出现。结构独特的节点是灰色的。第二个视图，它显示了不同操作运行的设备。名称范围与其内部操作的设备成比例。</p>
<p>下面的图片给出了例子。</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4kgyk7epj20tq0h777t.jpg"><br></center>


<h2 id="张量形状信息"><a href="#张量形状信息" class="headerlink" title="张量形状信息"></a>张量形状信息</h2><p>当序列化的<code>GraphDef</code>包含张量形状时，图形可视化器将张量标注为边缘，边缘厚度反映总张量大小。 要在<code>GraphDef</code>中包含张量形状，在序列化图形时将实际图形对象（如<code>sess.graph</code>）传递给<code>FileWriter</code>。 下面的图片显示了具有张量形状信息的CIFAR-10模型：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4kgz1b12j20az050mxt.jpg"><br></center>


<h2 id="运行时统计"><a href="#运行时统计" class="headerlink" title="运行时统计"></a>运行时统计</h2><p>收集运行时元数据通常是非常有用的，例如总内存使用量，总计算时间和节点的张量形状。 下面的代码示例是简单的MNIST教程的修改的训练和测试部分的一个片段，其中我们记录了摘要和运行时统计信息。 有关如何记录摘要的详细信息，请参阅摘要教程。 完整的源代码在这里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the model, and also write summaries.</span></span><br><span class="line"> <span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></span><br><span class="line"> <span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">feed_dict</span><span class="params">(train)</span>:</span></span><br><span class="line">   <span class="string">"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""</span></span><br><span class="line">   <span class="keyword">if</span> train <span class="keyword">or</span> FLAGS.fake_data:</span><br><span class="line">     xs, ys = mnist.train.next_batch(<span class="number">100</span>, fake_data=FLAGS.fake_data)</span><br><span class="line">     k = FLAGS.dropout</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">     xs, ys = mnist.test.images, mnist.test.labels</span><br><span class="line">     k = <span class="number">1.0</span></span><br><span class="line">   <span class="keyword">return</span> &#123;x: xs, y_: ys, keep_prob: k&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> range(FLAGS.max_steps):</span><br><span class="line">   <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># Record summaries and test-set accuracy</span></span><br><span class="line">     summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(<span class="keyword">False</span>))</span><br><span class="line">     test_writer.add_summary(summary, i)</span><br><span class="line">     print(<span class="string">'Accuracy at step %s: %s'</span> % (i, acc))</span><br><span class="line">   <span class="keyword">else</span>:  <span class="comment"># Record train set summaries, and train</span></span><br><span class="line">     <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">99</span>:  <span class="comment"># Record execution stats</span></span><br><span class="line">       run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</span><br><span class="line">       run_metadata = tf.RunMetadata()</span><br><span class="line">       summary, _ = sess.run([merged, train_step],</span><br><span class="line">                             feed_dict=feed_dict(<span class="keyword">True</span>),</span><br><span class="line">                             options=run_options,</span><br><span class="line">                             run_metadata=run_metadata)</span><br><span class="line">       train_writer.add_run_metadata(run_metadata, <span class="string">'step%d'</span> % i)</span><br><span class="line">       train_writer.add_summary(summary, i)</span><br><span class="line">       print(<span class="string">'Adding run metadata for'</span>, i)</span><br><span class="line">     <span class="keyword">else</span>:  <span class="comment"># Record a summary</span></span><br><span class="line">       summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(<span class="keyword">True</span>))</span><br><span class="line">       train_writer.add_summary(summary, i)</span><br></pre></td></tr></table></figure>
<p>此代码将从步骤99开始每100步发出运行时统计信息。</p>
<p>当启动tensorboard并转到图表选项卡时，您将在“会话运行”下看到与添加运行元数据的步骤相对应的选项。 选择其中一个运行将显示在该步骤的网络快照，淡出未使用的节点。 在左侧的控件中，您可以通过总内存或总计算时间对节点着色。 此外，单击节点将显示确切的总内存，计算时间和张量输出大小。</p>
<center class="half"><br><img src="http://wx3.sinaimg.cn/mw690/af2d2659ly1fm4kgzet29j20tt0g2dja.jpg"><br></center>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/estimator/">estimator</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-05-TensorBoard可视化学习"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/05/2017-12-05-TensorBoard可视化学习/">TensorBoard可视化学习</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/05/2017-12-05-TensorBoard可视化学习/" class="article-date">
	  <time datetime="2017-12-04T16:00:00.000Z" itemprop="datePublished">十二月 5, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>您将使用TensorFlow进行的计算 - 如训练大量的深度神经网络 - 可能会很复杂且令人困惑。 为了便于理解，调试和优化TensorFlow程序，我们包含了一套名为TensorBoard的可视化工具。 您可以使用TensorBoard来显示您的TensorFlow图形，绘制关于图形执行的量化指标，并显示其他数据，如图像。 当完全配置TensorBoard时，看起来像这样：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm4jyazdrfj20pw0dy77c.jpg"><br></center>


<h2 id="序列化数据"><a href="#序列化数据" class="headerlink" title="序列化数据"></a>序列化数据</h2><p>TensorBoard通过读取TensorFlow事件文件进行操作，TensorFlow事件文件包含运行TensorFlow时可以生成的摘要数据。以下是TensorBoard中汇总数据的一般生命周期。</p>
<p>首先，创建您希望从中收集摘要数据的TensorFlow图，然后决定使用摘要操作注解哪些节点。</p>
<p>例如，假设您正在训练用于识别MNIST数字的卷积神经网络。您想记录学习率随时间的变化，以及目标函数如何变化。通过将<code>tf.summary.scalar</code> 操作分别附加到输出学习率和损失来收集这些信息。然后，给每个标量附上有意义的标签，如“学习率”或“损失函数”。</p>
<p>也许你也想看到特定层的激活分布，或梯度或权重的分布。通过将<code>tf.summary.histogram</code>操作附加到梯度输出和保存您权重的变量来收集这些数据。</p>
<p>有关所有可用摘要操作的详细信息，请查看有关摘要操作的文档。</p>
<p>在运行之前，TensorFlow中不会执行任何操作，或者取决于其输出的操作。我们刚刚创建的摘要节点是图形的外围设备：您当前正在运行的操作都不依赖于它们。所以，为了生成摘要，我们需要运行所有这些汇总节点。手工管理它们会很麻烦，所以使用<code>tf.summary.merge_all</code>将它们组合成一个单独的操作来生成所有的汇总数据。</p>
<p>然后，您可以运行合并的摘要操作，该操作将在给定的步骤中生成包含所有摘要数据的序列化的protobuf对象。最后，为了将这个总结数据写入磁盘，将总结的protobuf传递给<code>tf.summary.FileWriter</code>。</p>
<p><code>FileWriter</code>在其构造函数中使用了一个<code>logdir</code> - 这个<code>logdir</code>非常重要，它是所有事件将被写出的目录。另外，<code>FileWriter</code>可以选择在其构造函数中使用Graph。如果它接收到一个Graph对象，那么TensorBoard会将您的图形与张量形状信息一起可视化。这将使您更好地理解图中流动的情况：请参阅张量形状信息。</p>
<p>现在你已经修改了你的图形，并有一个<code>FileWriter</code>，你已经准备好开始运行你的网络！如果你愿意，你可以每一步都运行合并的摘要操作，并记录大量的训练数据，虽然这可能产生更多的数据。所以请考虑每n步运行合并的摘要操作。</p>
<p>下面的代码示例是对简单MNIST教程的修改，我们在其中添加了一些汇总操作，并且每十步执行一次。如果你运行这个程序，然后启动<code>tensorboard --logdir = /tmp/tensorflow/mnist</code>，你将可以看到统计数据，例如训练过程中权重或精度的变化。下面的代码是摘录，完整的源代码在<a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py" target="_blank" rel="noopener">这里</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var)</span>:</span></span><br><span class="line">  <span class="string">"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</span><br><span class="line">    mean = tf.reduce_mean(var)</span><br><span class="line">    tf.summary.scalar(<span class="string">'mean'</span>, mean)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</span><br><span class="line">      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">    tf.summary.scalar(<span class="string">'stddev'</span>, stddev)</span><br><span class="line">    tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))</span><br><span class="line">    tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))</span><br><span class="line">    tf.summary.histogram(<span class="string">'histogram'</span>, var)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_layer</span><span class="params">(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu)</span>:</span></span><br><span class="line">  <span class="string">"""Reusable code for making a simple neural net layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  It does a matrix multiply, bias add, and then uses relu to nonlinearize.</span></span><br><span class="line"><span class="string">  It also sets up name scoping so that the resultant graph is easy to read,</span></span><br><span class="line"><span class="string">  and adds a number of summary ops.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Adding a name scope ensures logical grouping of the layers in the graph.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line">    <span class="comment"># This Variable will hold the state of the weights for the layer</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">      weights = weight_variable([input_dim, output_dim])</span><br><span class="line">      variable_summaries(weights)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">      biases = bias_variable([output_dim])</span><br><span class="line">      variable_summaries(biases)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">      preactivate = tf.matmul(input_tensor, weights) + biases</span><br><span class="line">      tf.summary.histogram(<span class="string">'pre_activations'</span>, preactivate)</span><br><span class="line">    activations = act(preactivate, name=<span class="string">'activation'</span>)</span><br><span class="line">    tf.summary.histogram(<span class="string">'activations'</span>, activations)</span><br><span class="line">    <span class="keyword">return</span> activations</span><br><span class="line"></span><br><span class="line">hidden1 = nn_layer(x, <span class="number">784</span>, <span class="number">500</span>, <span class="string">'layer1'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'dropout'</span>):</span><br><span class="line">  keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">  tf.summary.scalar(<span class="string">'dropout_keep_probability'</span>, keep_prob)</span><br><span class="line">  dropped = tf.nn.dropout(hidden1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not apply softmax activation yet, see below.</span></span><br><span class="line">y = nn_layer(dropped, <span class="number">500</span>, <span class="number">10</span>, <span class="string">'layer2'</span>, act=tf.identity)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy'</span>):</span><br><span class="line">  <span class="comment"># The raw formulation of cross-entropy,</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),</span></span><br><span class="line">  <span class="comment">#                               reduction_indices=[1]))</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># can be numerically unstable.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># So here we use tf.nn.softmax_cross_entropy_with_logits on the</span></span><br><span class="line">  <span class="comment"># raw outputs of the nn_layer above, and then average across</span></span><br><span class="line">  <span class="comment"># the batch.</span></span><br><span class="line">  diff = tf.nn.softmax_cross_entropy_with_logits(targets=y_, logits=y)</span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">'total'</span>):</span><br><span class="line">    cross_entropy = tf.reduce_mean(diff)</span><br><span class="line">tf.summary.scalar(<span class="string">'cross_entropy'</span>, cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">  train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(</span><br><span class="line">      cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge all the summaries and write them out to /tmp/mnist_logs (by default)</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + <span class="string">'/train'</span>,</span><br><span class="line">                                      sess.graph)</span><br><span class="line">test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + <span class="string">'/test'</span>)</span><br><span class="line">tf.global_variables_initializer().run()</span><br></pre></td></tr></table></figure>
<p>在初始化<code>FileWriters</code>之后，我们必须在我们训练和测试模型时将摘要添加到<code>FileWriters</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the model, and also write summaries.</span></span><br><span class="line"><span class="comment"># Every 10th step, measure test-set accuracy, and write test summaries</span></span><br><span class="line"><span class="comment"># All other steps, run train_step on training data, &amp; add training summaries</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_dict</span><span class="params">(train)</span>:</span></span><br><span class="line">  <span class="string">"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""</span></span><br><span class="line">  <span class="keyword">if</span> train <span class="keyword">or</span> FLAGS.fake_data:</span><br><span class="line">    xs, ys = mnist.train.next_batch(<span class="number">100</span>, fake_data=FLAGS.fake_data)</span><br><span class="line">    k = FLAGS.dropout</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    xs, ys = mnist.test.images, mnist.test.labels</span><br><span class="line">    k = <span class="number">1.0</span></span><br><span class="line">  <span class="keyword">return</span> &#123;x: xs, y_: ys, keep_prob: k&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(FLAGS.max_steps):</span><br><span class="line">  <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># Record summaries and test-set accuracy</span></span><br><span class="line">    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(<span class="keyword">False</span>))</span><br><span class="line">    test_writer.add_summary(summary, i)</span><br><span class="line">    print(<span class="string">'Accuracy at step %s: %s'</span> % (i, acc))</span><br><span class="line">  <span class="keyword">else</span>:  <span class="comment"># Record train set summaries, and train</span></span><br><span class="line">    summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(<span class="keyword">True</span>))</span><br><span class="line">    train_writer.add_summary(summary, i)</span><br></pre></td></tr></table></figure>
<p>您现在已经可以使用TensorBoard将这些数据可视化了。</p>
<h2 id="启动TensorBoard"><a href="#启动TensorBoard" class="headerlink" title="启动TensorBoard"></a>启动TensorBoard</h2><p>要运行TensorBoard，请使用以下命令（或者使用<code>python -m tensorboard.main</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=path/to/log-directory</span><br></pre></td></tr></table></figure>
<p>其中<code>logdir</code>指向<code>FileWriter</code>序列化其数据的目录。 如果此<code>logdir</code>目录包含来自单独运行的序列化数据的子目录，则TensorBoard将可视化来自所有这些运行的数据。 一旦TensorBoard正在运行，浏览您的Web浏览器到<code>localhost:6006</code>以查看TensorBoard。</p>
<p>在看TensorBoard时，您会看到右上角的导航标签。 每个选项卡代表一组可以可视化的序列化数据。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/estimator/">estimator</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-04-tf.estimator-快速上手"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/04/2017-12-04-tf.estimator-快速上手/">tf.estimator 快速上手</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/04/2017-12-04-tf.estimator-快速上手/" class="article-date">
	  <time datetime="2017-12-03T16:00:00.000Z" itemprop="datePublished">十二月 4, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>TensorFlow的高级机器学习API（tf.estimator）可以轻松配置，训练和评估各种机器学习模型。 在本教程中，您将使用tf.estimator构建一个神经网络分类器，并在Iris数据集上对其进行训练，以基于萼片/花瓣几何学来预测花朵种类。 您将编写代码来执行以下五个步骤：</p>
<ul>
<li>将包含Iris训练/测试数据的CSV加载到TensorFlow数据集中</li>
<li>构建一个神经网络分类器</li>
<li>使用训练数据训练模型</li>
<li>评估模型的准确性</li>
<li>分类新样品</li>
</ul>
<p>注：在开始本教程之前，请记住在您的机器上安装TensorFlow。</p>
<h2 id="完整的神经网络源代码"><a href="#完整的神经网络源代码" class="headerlink" title="完整的神经网络源代码"></a>完整的神经网络源代码</h2><p>以下是神经网络分类器的完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> six.moves.urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data sets</span></span><br><span class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></span><br><span class="line">IRIS_TRAINING_URL = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></span><br><span class="line"></span><br><span class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></span><br><span class="line">IRIS_TEST_URL = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># If the training and test sets aren't stored locally, download them.</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TRAINING):</span><br><span class="line">    raw = urlopen(IRIS_TRAINING_URL).read()</span><br><span class="line">    <span class="keyword">with</span> open(IRIS_TRAINING, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">      f.write(raw)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TEST):</span><br><span class="line">    raw = urlopen(IRIS_TEST_URL).read()</span><br><span class="line">    <span class="keyword">with</span> open(IRIS_TEST, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">      f.write(raw)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Load datasets.</span></span><br><span class="line">  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">      filename=IRIS_TRAINING,</span><br><span class="line">      target_dtype=np.int,</span><br><span class="line">      features_dtype=np.float32)</span><br><span class="line">  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">      filename=IRIS_TEST,</span><br><span class="line">      target_dtype=np.int,</span><br><span class="line">      features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">  feature_columns = [tf.feature_column.numeric_column(<span class="string">"x"</span>, shape=[<span class="number">4</span>])]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,</span><br><span class="line">                                          hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">                                          n_classes=<span class="number">3</span>,</span><br><span class="line">                                          model_dir=<span class="string">"/tmp/iris_model"</span>)</span><br><span class="line">  <span class="comment"># Define the training inputs</span></span><br><span class="line">  train_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">      x=&#123;<span class="string">"x"</span>: np.array(training_set.data)&#125;,</span><br><span class="line">      y=np.array(training_set.target),</span><br><span class="line">      num_epochs=<span class="keyword">None</span>,</span><br><span class="line">      shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train model.</span></span><br><span class="line">  classifier.train(input_fn=train_input_fn, steps=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Define the test inputs</span></span><br><span class="line">  test_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">      x=&#123;<span class="string">"x"</span>: np.array(test_set.data)&#125;,</span><br><span class="line">      y=np.array(test_set.target),</span><br><span class="line">      num_epochs=<span class="number">1</span>,</span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate accuracy.</span></span><br><span class="line">  accuracy_score = classifier.evaluate(input_fn=test_input_fn)[<span class="string">"accuracy"</span>]</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Classify two new flower samples.</span></span><br><span class="line">  new_samples = np.array(</span><br><span class="line">      [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</span><br><span class="line">       [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</span><br><span class="line">  predict_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">      x=&#123;<span class="string">"x"</span>: new_samples&#125;,</span><br><span class="line">      num_epochs=<span class="number">1</span>,</span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  predictions = list(classifier.predict(input_fn=predict_input_fn))</span><br><span class="line">  predicted_classes = [p[<span class="string">"classes"</span>] <span class="keyword">for</span> p <span class="keyword">in</span> predictions]</span><br><span class="line"></span><br><span class="line">  print(</span><br><span class="line">      <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></span><br><span class="line">      .format(predicted_classes))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>以下部分详细介绍了代码。</p>
<h2 id="将Iris-CSV数据加载到TensorFlow"><a href="#将Iris-CSV数据加载到TensorFlow" class="headerlink" title="将Iris CSV数据加载到TensorFlow"></a>将Iris CSV数据加载到TensorFlow</h2><p>Iris数据集包含150行数据，包括来自三个相关鸢尾属物种中的每一个的50个样品：Iris setosa，Iris virginica和Iris versicolor。</p>
<center class="half"><br><img src="http://wx2.sinaimg.cn/mw690/af2d2659ly1fm3s9nusftj20t908jdvv.jpg"><br></center>

<p><strong>From left to right, Iris setosa (by Radomil, CC BY-SA 3.0), Iris versicolor (by Dlanglois, CC BY-SA 3.0), and Iris virginica(by Frank Mayfield, CC BY-SA 2.0).</strong></p>
<p>每行包含每个花样的以下数据：萼片长度，萼片宽度，花瓣长度，花瓣宽度和花种。 花种以整数表示，其中0表示Iris setosa，1表示Iris virginica，2表示Iris versicolor。</p>
<p>对于本教程，Iris数据已被随机分成两个独立的CSV：</p>
<ul>
<li>120个样本的训练集（<a href="http://download.tensorflow.org/data/iris_training.csv" target="_blank" rel="noopener">iris_training.csv</a>）</li>
<li>30个样本的测试集（<a href="http://download.tensorflow.org/data/iris_test.csv" target="_blank" rel="noopener">iris_test.csv</a>）。</li>
</ul>
<p>开始前，首先导入所有必要的模块，并定义下载和存储数据集的位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> six.moves.urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></span><br><span class="line">IRIS_TRAINING_URL = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></span><br><span class="line"></span><br><span class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></span><br><span class="line">IRIS_TEST_URL = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br></pre></td></tr></table></figure>
<p>然后，如果训练和测试集尚未存储在本地，请下载它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TRAINING):</span><br><span class="line">  raw = urlopen(IRIS_TRAINING_URL).read()</span><br><span class="line">  <span class="keyword">with</span> open(IRIS_TRAINING,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(raw)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(IRIS_TEST):</span><br><span class="line">  raw = urlopen(IRIS_TEST_URL).read()</span><br><span class="line">  <span class="keyword">with</span> open(IRIS_TEST,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(raw)</span><br></pre></td></tr></table></figure>
<p>接下来，使用<code>learn.datasets.base</code>中的<code>load_csv_with_header()</code>方法将训练集和测试集加载到数据集中。 <code>load_csv_with_header()</code>方法需要三个必需的参数：</p>
<ul>
<li><code>filename</code>，它将文件路径转换为CSV文件</li>
<li><code>target_dtype</code>，它采用数据集的目标值的numpy数据类型。</li>
<li><code>features_dtype</code>，它采用数据集特征值的numpy数据类型。</li>
</ul>
<p>在这里，目标（你正在训练模型来预测的值）是花的种类，它是一个从0到2的整数，所以合适的numpy数据类型是np.int：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load datasets.</span></span><br><span class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TRAINING,</span><br><span class="line">    target_dtype=np.int,</span><br><span class="line">    features_dtype=np.float32)</span><br><span class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TEST,</span><br><span class="line">    target_dtype=np.int,</span><br><span class="line">    features_dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p><code>tf.contrib.learn</code>中的数据集被命名为元组;您可以通过<code>data</code>和<code>target</code>字段访问特征数据和目标值。这里，<code>training_set.data</code>和<code>training_set.target</code>分别包含训练集的特征数据和目标值，<code>test_set.data</code>和<code>test_set.target</code>包含测试集的特征数据和目标值。</p>
<p>稍后，在“将DNNClassifier安装到Iris训练数据”中，您将使用<code>training_set.data</code>和<code>training_set.target</code>来训练您的模型，在“Evaluate Model Accuracy”中，您将使用<code>test_set.data</code>和<code>test_set.target</code>。但首先，您将在下一节中构建您的模型。</p>
<h2 id="构建深度神经网络分类器"><a href="#构建深度神经网络分类器" class="headerlink" title="构建深度神经网络分类器"></a>构建深度神经网络分类器</h2><p>tf.estimator提供了各种预定义的模型，称为<code>Estimators</code>，您可以使用“开箱即用”对数据进行训练和评估操作。在这里，您将配置深度神经网络分类器模型以适应Iris数据。使用tf.estimator，你可以用几行代码实例化你的<code>tf.estimator.DNNClassifier</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">feature_columns = [tf.feature_column.numeric_column(<span class="string">"x"</span>, shape=[<span class="number">4</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,</span><br><span class="line">                                        hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">                                        n_classes=<span class="number">3</span>,</span><br><span class="line">                                        model_dir=<span class="string">"/tmp/iris_model"</span>)</span><br></pre></td></tr></table></figure>
<p>上面的代码首先定义模型的特征列，它指定数据集中特征的数据类型。所有的特征数据都是连续的，所以<code>tf.feature_column.numeric_column</code>是用来构造特征列的适当函数。数据集中有四个特征（萼片宽度，萼片高度，花瓣宽度和花瓣高度），所以相应的形状必须设置为<code>[4]</code>来保存所有的数据。</p>
<p>然后，代码使用以下参数创建一个DNNClassifier模型：</p>
<ul>
<li><code>feature_columns = feature_columns</code>。上面定义的一组特征列。</li>
<li><code>hidden_units = [10，20，10]</code>。三个隐藏层，分别包含10,20和10个神经元。</li>
<li><code>n_classes = 3</code>。三个目标类，代表三个鸢尾属。</li>
<li><code>model_dir =/tmp/iris_model</code>。 TensorFlow将在模型训练期间保存检查点数据和TensorBoard摘要的目录。</li>
</ul>
<h2 id="描述训练输入管道"><a href="#描述训练输入管道" class="headerlink" title="描述训练输入管道"></a>描述训练输入管道</h2><p><code>tf.estimator</code> API使用输入函数，这些输入函数创建了用于为模型生成数据的TensorFlow操作。我们可以使用<code>tf.estimator.inputs.numpy_input_fn</code>来产生输入管道：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the training inputs</span></span><br><span class="line">train_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">"x"</span>: np.array(training_set.data)&#125;,</span><br><span class="line">    y=np.array(training_set.target),</span><br><span class="line">    num_epochs=<span class="keyword">None</span>,</span><br><span class="line">    shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>将DNNClassifier安装到Iris训练数据</p>
<p>现在，您已经配置了DNN分类器模型，可以使用<code>train</code>方法将其适用于Iris训练数据。 将<code>train_input_fn</code>传递给<code>input_fn</code>，以及要训练的步数（这里是2000）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train model.</span></span><br><span class="line">classifier.train(input_fn=train_input_fn, steps=<span class="number">2000</span>)</span><br></pre></td></tr></table></figure>
<p>模型的状态保存在分类器中，这意味着如果你喜欢，可以迭代地训练。 例如，上面的做法相当于以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classifier.train(input_fn=train_input_fn, steps=<span class="number">1000</span>)</span><br><span class="line">classifier.train(input_fn=train_input_fn, steps=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>但是，如果您希望在训练时跟踪模型，则可能需要使用TensorFlow <code>SessionRunHook</code>来执行日志记录操作。</p>
<h2 id="评估模型的准确性"><a href="#评估模型的准确性" class="headerlink" title="评估模型的准确性"></a>评估模型的准确性</h2><p>您已经在Iris训练数据上训练了您的<code>DNNClassifier</code>模型; 现在，您可以使用评估方法检查Iris测试数据的准确性。 像<code>train</code>一样，<code>evaluate</code>需要一个输入函数来建立它的输入流水线。 评估返回与评估结果的字典。 以下代码将通过Iris测试<code>data-test_set.data</code>和<code>test_set.target</code>来评估和打印结果的准确性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the test inputs</span></span><br><span class="line">test_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">"x"</span>: np.array(test_set.data)&#125;,</span><br><span class="line">    y=np.array(test_set.target),</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate accuracy.</span></span><br><span class="line">accuracy_score = classifier.evaluate(input_fn=test_input_fn)[<span class="string">"accuracy"</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nTest Accuracy: &#123;0:f&#125;\n"</span>.format(accuracy_score))</span><br></pre></td></tr></table></figure>
<p>注意：这里numpy_input_fn的num_epochs = 1参数很重要。 test_input_fn将迭代数据一次，然后引发OutOfRangeError。 这个错误表示分类器停止评估，所以它会在输入上评估一次。</p>
<p>当你运行完整的脚本时，它会打印出一些接近的内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Test Accuracy: <span class="number">0.966667</span></span><br></pre></td></tr></table></figure>
<p>您的准确性结果可能会有所不同，但应该高于90％。 对于相对较小的数据集来说很不错了！</p>
<h2 id="分类新样品"><a href="#分类新样品" class="headerlink" title="分类新样品"></a>分类新样品</h2><p>使用估计器的<code>predict()</code>方法对新样本进行分类。 例如，假设你有这两个新的花样：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm3s9ose8dj20tq046q2w.jpg"><br></center>

<p>您可以使用<code>predict()</code>方法预测它们的物种。 预测返回一个字符串生成器，可以很容易地转换为列表。 以下代码检索并打印类预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Classify two new flower samples.</span></span><br><span class="line">new_samples = np.array(</span><br><span class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>],</span><br><span class="line">     [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=np.float32)</span><br><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">"x"</span>: new_samples&#125;,</span><br><span class="line">    num_epochs=<span class="number">1</span>,</span><br><span class="line">    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">predictions = list(classifier.predict(input_fn=predict_input_fn))</span><br><span class="line">predicted_classes = [p[<span class="string">"classes"</span>] <span class="keyword">for</span> p <span class="keyword">in</span> predictions]</span><br><span class="line"></span><br><span class="line">print(</span><br><span class="line">    <span class="string">"New Samples, Class Predictions:    &#123;&#125;\n"</span></span><br><span class="line">    .format(predicted_classes))</span><br></pre></td></tr></table></figure>
<p>你的结果应该如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">New Samples, Class Predictions:    [<span class="number">1</span> <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>因此，模型预测第一个样品是<em>Iris versicolor</em>，第二个样品是<em>Iris virginica</em>。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/estimator/">estimator</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  
    <article id="post-2017-12-03-深入研究MNIST"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2017/12/03/2017-12-03-深入研究MNIST/">深入研究MNIST</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2017/12/03/2017-12-03-深入研究MNIST/" class="article-date">
	  <time datetime="2017-12-02T16:00:00.000Z" itemprop="datePublished">十二月 3, 2017</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
 
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="加载MNIST数据"><a href="#加载MNIST数据" class="headerlink" title="加载MNIST数据"></a>加载MNIST数据</h2><p>如果您正在复制和粘贴本教程的代码，请从这里开始，下面这两行代码将自动下载并读取数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里mnist是一个轻量级数据集，它将训练、验证和测试集存储为NumPy数组。 它还提供了一个函数来迭代数据minibatches，我们将在下面使用。</p>
<h2 id="启动TensorFlow-InteractiveSession"><a href="#启动TensorFlow-InteractiveSession" class="headerlink" title="启动TensorFlow InteractiveSession"></a>启动TensorFlow InteractiveSession</h2><p>TensorFlow依靠高效的C ++后端来完成它的计算。 与此后端的连接称为会话。 TensorFlow程序的常见用法是首先创建一个图形，然后在会话中启动它。</p>
<p>在这里，我们使用便利的<code>InteractiveSession</code>类，这使得TensorFlow更加灵活地了解如何构建代码。 它允许您将构建计算图的操作与运行图的操作交错。 在IPython等交互式环境中工作时，这特别方便。 如果您没有使用<code>InteractiveSession</code>，则应在开始会话并启动图之前构建整个计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>为了在Python中进行有效的数值计算，我们通常使用像NumPy这样的库来执行复杂的操作，例如Python之外的矩阵乘法，使用另一种语言实现的高效代码来完成这些操作。不幸的是，每一次操作都会返回到Python，这仍然会有很多开销。如果要在GPU上运行计算或以分布式方式运行计算，则传输数据的成本很高，所以此开销尤其糟糕。</p>
<p>TensorFlow也在Python之外进行繁重的工作，但是为了避免这种开销，还需要进一步的工作。 TensorFlow不是独立于Python运行的一个复杂操作，而是让我们描述一个完全在Python之外运行的交互操作图。 （像这样的方法可以在几个机器学习库中看到）</p>
<p>因此，Python代码的作用是构建这个外部计算图，并指定运行图的每个部分。</p>
<p>#建立一个Softmax回归模型</p>
<p>在本节中，我们将建立一个单线性层的softmax回归模型。在下一节中，我们将扩展到具有多层卷积网络的softmax回归的情况。</p>
<h2 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h2><p>我们通过为输入图像和目标输出类创建节点来开始构建计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>这里<code>x</code>和<code>y_</code>不是特定的值。相反，它们都是占位符 - 当我们要求TensorFlow运行一个计算时，我们会输入一个值。</p>
<p>输入图像<code>x</code>将由浮点数的二维张量组成。这里我们给它赋予一个<code>[None，784]</code>的形状，其中<code>784</code>是一个单一的28×28像素MNIST图像的维度，<code>None</code>表示与batch大小相对应的第一维度可以是任意大小。目标输出类别<code>y_</code>也将由二维张量组成，其中每一行是一个唯一的10维向量，指示对应的MNIST图像是哪个数字（0到9）。</p>
<p>占位符的形状参数是可选的，但它允许TensorFlow自动捕捉源自不一致张量形状的错误。</p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>我们现在定义权重<code>W</code>，并为我们的模型赋予偏差<code>b</code>。我们可以把这些看作是额外的输入，但是TensorFlow有一个更好的方法来处理它们：Variable（变量）。变量是存在于TensorFlow的计算图中的一个值。它可以被使用，甚至被计算修改。在机器学习应用中，通常将模型参数设置为变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>我们将调用中的每个参数的初始值传递给<code>tf.Variable</code>。 在这种情况下，我们将<code>W</code>和<code>b</code>初始化为全0的张量。 W是784x10矩阵（因为我们有784个输入特征和10个输出），<code>b</code>是10维向量（因为我们有10个数字）。</p>
<p>在会话中使用变量之前，必须使用该会话进行初始化变量。 这一步将已经指定的初始值（在这里，张量全零），分配给每个变量。 这可以一次完成所有变量的赋值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<h2 id="预测类和损失函数"><a href="#预测类和损失函数" class="headerlink" title="预测类和损失函数"></a>预测类和损失函数</h2><p>我们现在可以实现我们的回归模型。 只需要一行！ 我们将向量化的输入图像<code>x</code>乘以权重矩阵<code>W</code>，加上偏差<code>b</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul(x,W) + b</span><br></pre></td></tr></table></figure>
<p>我们可以很容易地指定一个损失函数。 损失函数描述模型的预测值在一个例子上有多差; 在训练所有的例子时，我们尽量减少损失函数的值。 在这里，我们的损失函数是目标和应用于模型预测的softmax激活函数之间的交叉熵。 正如在初学者教程中，我们使用同样的公式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</span><br></pre></td></tr></table></figure>
<p>请注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>在模型的非标准化模型预测中内部应用softmax，并在所有类中进行求和，<code>tf.reduce_mean</code>取这些和的平均值。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在我们已经定义了我们的模型和训练损失函数，接下来使用TensorFlow进行训练是很简单的。 由于TensorFlow知道整个计算图，因此可以使用自动微分来查找相对于每个变量的损失的梯度。 TensorFlow有多种内置的优化算法。 对于这个例子，我们将使用最陡的梯度下降，步长为0.5，降低交叉熵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>TensorFlow在这一行中实际上是在计算图中添加新的操作。 这些操作包括计算梯度、计算参数、更新步骤以及将更新步骤应用于参数。</p>
<p>运行<code>train_step</code>时返回的参数会用于更新梯度下降。 因此，训练模型可以通过重复运行<code>train_step</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>我们在每次训练迭代中加载100个训练样例。 然后我们运行<code>train_step</code>操作，使用<code>feed_dict</code>将训练样例中的占位符张量<code>x</code>和<code>y_</code>替换。 请注意，您可以使用<code>feed_dict</code>来替换计算图中的任何张量 - 它不仅限于占位符。</p>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>我们的模型有多好？</p>
<p>首先我们要弄清楚我们在哪里预测了正确的标签。 <code>tf.argmax</code>是一个非常有用的函数，它可以为您提供某个轴上张量的最大输入索引。 例如，<code>tf.argmax(y，1)</code>是我们模型认为对每个输入最有可能的标签，而<code>tf.argmax(y_，1)</code>是真正的标签。 我们可以使用<code>tf.equal</code>来检查我们的预测是否符合事实。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这里给出一个真值表。 为了确定什么分数是正确的，我们将布尔值转换为浮点数，然后取平均值。 例如，<code>[True，False，True，True]</code>将变成<code>[1,0,1,1]</code>，这将变为<code>0.75</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>最后，我们可以评估我们的测试数据的准确性。 这里应该是大约92％的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>#构建一个多层卷积网络</p>
<p>在MNIST上获得92％的准确性是不好的。 这几乎是令人尴尬的坏事。 在这一节中，我们将解决这个问题，从一个非常简单的模型跳到一些中等复杂的问题：一个小的卷积神经网络。 这将使我们达到约99.2％的准确性 - 不是最先进的，但是值得学习。</p>
<p>下面是一个用TensorBoard创建的关于我们将要构建的模型的图表：</p>
<center class="half"><br><img src="http://wx1.sinaimg.cn/mw690/af2d2659ly1fm3dvcsajhj20c20qltan.jpg"><br></center>


<h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>要创建这个模型，我们需要创建很多权重和偏差。 一般应该用少量的噪声初始化权重，以防止对称性破坏，并防止0梯度。 由于我们使用的是ReLU神经元，为了避免“死神经元”，初始化这些神经元是一个很好的做法。 在我们构建模型的时候不要重复这样的操作，而是创建两个函数来为我们做这件事。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h2 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h2><p>TensorFlow也为卷积和池化操作提供了很大的灵活性。 我们如何处理边界？ 我们的步幅是多少？ 在这个例子中，我们选择vanilla版本。 我们使步幅大小为1，并在周围填充零，以便输出与输入大小相同。 我们的pooling是超过2x2的max pooling。 为了保证我们的代码更清晰，我们也将这些操作抽象为函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<p>tf.nn.conv2d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=<span class="keyword">True</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>input：张量。必须是以下类型之一：half，float32。一个四维张量。维度顺序根据data_format的值来解释，详见下文。</li>
<li>filter：张量。必须具有与输入相同的类型。形状的四维张量[filter_height，filter_width，in_channels，out_channels]</li>
<li>strides：整数列表。一维长度的张量4.输入每个维度的滑动窗口的步幅。维度顺序由data_format的值决定，详见下文。</li>
<li>padding：来自“SAME”，“VALID”的字符串。要使用的填充算法的类型。</li>
<li>use_cudnn_on_gpu：一个可选的布尔。默认为True。</li>
<li>data_format：来自“NHWC”，“NCHW”的可选字符串。默认为“NHWC”。指定输入和输出数据的数据格式。使用默认格式“NHWC”，数据按照[batch，height，width，channels]的顺序存储。或者，格式可以是“NCHW”，数据存储顺序为：[batch，channels，height，width]。</li>
<li>name：操作的名称（可选）。</li>
</ul>
<p>tf.nn.max_pool</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">max_pool(</span><br><span class="line">    value,</span><br><span class="line">    ksize,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>value：由data_format指定的格式的4维张量。</li>
<li>ksize：4元素的1维 int型张量。 输入张量表示窗口每个维度的大小。</li>
<li>strides：4元素的1维int型张量。 输入张量表示滑动窗口每个维度的步幅。</li>
<li>padding：一个字符串，可以是’VALID’ 或 ‘SAME’。</li>
<li>data_format：一个字符串。 支持“NHWC”，“NCHW”和“NCHW_VECT_C”。</li>
<li>name：操作的可选名称。</li>
</ul>
<h2 id="第一卷积层"><a href="#第一卷积层" class="headerlink" title="第一卷积层"></a>第一卷积层</h2><p>我们现在可以实现我们的第一层。 它将由卷积组成，然后是max pooling。 卷积将为每个5x5 patch计算32个特征。 它的权重张量是<code>[5,5,1,32]</code>的形状。 前两个维度是patch大小，下一个是输入通道的数量，最后一个是输出通道的数量。 每个输出通道还会有带有一个偏差向量的分量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br></pre></td></tr></table></figure>
<p>为了应用该层，我们首先将<code>x</code>重塑为4维张量，第二维和第三维对应于图像的宽度和高度，并且最后一个维度对应于色彩通道的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>然后，我们将<code>x_image</code>与权重张量进行卷积，加上偏差，应用ReLU函数，最后使用max pooling。 <code>max_pool_2x2</code>方法将图像大小减小到14x14。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br></pre></td></tr></table></figure>
<h2 id="第二卷积层"><a href="#第二卷积层" class="headerlink" title="第二卷积层"></a>第二卷积层</h2><p>为了建立一个深层网络，我们堆叠了这种类型的几个层。 第二层将为每个5x5 patch有64个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h2 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h2><p>现在图像尺寸已经减小到7x7，我们添加了一个1024个神经元的全连接图层，以允许在整个图像上进行处理。 我们将pooling层中的张量重塑为一批向量，乘以权重矩阵，添加一个偏差，并应用一个ReLU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>为了减少过拟合，我们将在读出层之前应用dropout。 我们创建一个占位符，用于在dropout期间保持神经元输出的概率。 这可以让我们在训练过程中关闭dropout，并在测试过程中将其关闭。 TensorFlow的<code>tf.nn.dropout</code>可以自动处理缩放神经元输出和掩蔽它们，所以dropout只是在没有任何附加缩放的情况下工作。（对于这个小卷积网络，性能实际上几乎是相同的，没有丢失。 dropout对于减少过拟合通常是非常有效的，但是是在训练非常大的神经网络。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<p>tf.nn.dropout</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropout(</span><br><span class="line">    x,</span><br><span class="line">    keep_prob,</span><br><span class="line">    noise_shape=<span class="keyword">None</span>,</span><br><span class="line">    seed=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>x：浮点张量。</li>
<li>keep_prob：与x相同类型的张量。 每个元素被保留的概率。</li>
<li>noise_shape：int32类型的一维张量，表示随机生成的保留/丢弃标志。</li>
<li>seed：一个Python整数。 用于创建随机种子。 有关行为，请参阅tf.set_random_seed。</li>
<li>name：此操作的名称（可选）。</li>
</ul>
<p>对于概率<code>keep_prob</code>，输出按1 / keep_prob放大的输入元素，否则输出0。</p>
<h2 id="读出层"><a href="#读出层" class="headerlink" title="读出层"></a>读出层</h2><p>最后，我们添加一个图层，就像上面的一层softmax回归一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<h2 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h2><p>这个模型有多好？ 为了训练和评估，我们将使用与上述简单的一层SoftMax网络几乎相同的代码。</p>
<p>不同之处在于：</p>
<ul>
<li>我们将用更复杂的ADAM优化器替代最陡的梯度下降优化器。</li>
<li>我们将在<code>feed_dict</code>中包含附加参数<code>keep_prob</code>来控制丢失率。</li>
<li>我们将在训练过程中每100次迭代添加一次记录。</li>
</ul>
<p>我们也将使用<code>tf.Session</code>而不是<code>tf.InteractiveSession</code>。 这更好地分离了创建图（模型说明）的过程和评估图（模型拟合）的过程。 它通常使更清晰的代码。<code>tf.Session</code>是在一个块内创建的，所以一旦块退出，它就会被自动销毁。</p>
<p>运行这个代码。 请注意，它会进行20,000次训练迭代，可能需要一段时间（可能长达半小时），具体取决于您的处理器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">          x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">      print(<span class="string">'step %d, training accuracy %g'</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">'test accuracy %g'</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">      x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>运行此代码后的最终测试集精度应该约为99.2％。</p>
<p>我们已经学会了如何使用TensorFlow快速，轻松地构建，训练和评估相当复杂的深度学习模型。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MNIST/">MNIST</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>

      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
</article>

<!-- Table of Contents -->

  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>

</section>
          <aside id="sidebar">
  
    <div style="margin: 20px 0;">
	<div id="search-form-wrap">

    <form class="search-form">
        <label style="width: 72%;">
            <span class="screen-reader-text">Search for:</span>
            <input type="search" class="search-field" style="height: 36px; width: 100%;" placeholder=" 搜索…" value="" name="s" title="Search for:">
        </label>
        <input type="submit" class="search-form-submit" value="搜索">
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="请输入关键词..."/>
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(无标题)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">联系我们</h3>
    <div class="widget widget_athemes_social_icons">

    	<ul class="clearfix widget-social-icons">   
    	
   			<li><a href="https://github.com/iTimeTraveler" title="Github"><i class="fa fa-github" aria-hidden="true"></i></a></li> 

   		
   			<li><a href="" title="Weibo"><i class="fa fa-weibo" aria-hidden="true"></i></a></li> 

   		
   			<li><a href="" title="Twitter"><i class="fa fa-twitter" aria-hidden="true"></i></a></li> 

   		
   			<li><a href="" title="Facebook"><i class="fa fa-facebook" aria-hidden="true"></i></a></li> 

   		
   			<li><a href="" title="Google-plus"><i class="fa fa-google-plus" aria-hidden="true"></i></a></li> 

   		
   			<li><a href="" title="Instagram"><i class="fa fa-instagram" aria-hidden="true"></i></a></li> 

   		
   		</ul>


   		<!--
   		<ul class="clearfix widget-social-icons">   		
   		<li class="widget-si-twitter"><a href="http://twitter.com" title="Twitter"><i class="ico-twitter"></i></a></li> 
		<li class="widget-si-facebook"><a href="http://facebook.com" title="Facebook"><i class="ico-facebook"></i></a></li>
			<li class="widget-si-gplus"><a href="http://plus.google.com" title="Google+"><i class="ico-gplus"></i></a></li>
			<li class="widget-si-pinterest"><a href="http://pinterest.com" title="Pinterest"><i class="ico-pinterest"></i></a></li>
			<li class="widget-si-flickr"><a href="http://flickr.com" title="Flickr"><i class="ico-flickr"></i></a></li>
			<li class="widget-si-instagram"><a href="http://instagram.com" title="Instagram"><i class="ico-instagram"></i></a></li>
		</ul> -->

    </div>
  </div>


  
    
  <div class="widget_athemes_tabs">
    <ul id="widget-tab" class="clearfix widget-tab-nav">
      <li class="active"><a>最新文章</a></li>
    </ul>
    <div class="widget">
      <ul>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/20/2017-12-20-YOLOv2代码分析（四）/">YOLOv2代码分析（四）</a></h6>
              <span>十二月 20, 2017</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/19/2017-12-19-YOLOv2代码分析（三）/">YOLOv2代码分析（三）</a></h6>
              <span>十二月 19, 2017</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/17/2017-12-17-YOLOv2代码分析（二）/">YOLOv2代码分析（二）</a></h6>
              <span>十二月 17, 2017</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/16/2017-12-16-YOLOv2代码分析（一）/">YOLOv2代码分析（一）</a></h6>
              <span>十二月 16, 2017</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/15/2017-12-15-目标检测中的IOU计算问题/">目标检测中的IOU计算问题</a></h6>
              <span>十二月 15, 2017</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2017/12/13/2017-12-13-tf.identity的作用/">tf.identity的作用</a></h6>
              <span>十二月 13, 2017</span>
            </div>

          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Database/">Database</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/QT/">QT</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/asp/">asp</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/c/">c</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cpp/">cpp</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/index/">index</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/toy/">toy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/信息安全/">信息安全</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机系统原理/">计算机系统原理</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机网络/">计算机网络</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/1080ti/">1080ti</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Aggregate/">Aggregate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dell/">Dell</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Eassy/">Eassy</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Eval/">Eval</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Execute/">Execute</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ExecuteGlobal/">ExecuteGlobal</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fibonacci/">Fibonacci</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IOU/">IOU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kolakoski/">Kolakoski</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MNIST/">MNIST</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/">MySQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PIL/">PIL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/POD/">POD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QT/">QT</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/YOLO/">YOLO</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/acm/">acm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/advance/">advance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/array/">array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/asp/">asp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/auto/">auto</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/auto/">auto&</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/auto/">auto&&</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binding/">binding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/">c</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/">c#</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cin/">cin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/constexpr/">constexpr</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp/">cpp</a><span class="tag-list-count">90</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp标准库/">cpp标准库</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpp第二版/">cpp第二版</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cuda/">cuda</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cudnn/">cudnn</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/darknet/">darknet</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dd/">dd</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/default/">default</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/downcasting/">downcasting</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/error/">error</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/estimator/">estimator</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/forward/">forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/game/">game</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/glvalue/">glvalue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hook/">hook</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/identity/">identity</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lambda/">lambda</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lvalue/">lvalue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/map/">map</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mismatch/">mismatch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/move/">move</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/namehiding/">namehiding</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/network/">network</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/next/">next</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notes/">notes</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/office/">office</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oj/">oj</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/opencv/">opencv</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oracle/">oracle</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/overload/">overload</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/override/">override</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/php/">php</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/piecewise-construct/">piecewise_construct</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pipe/">pipe</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/process/">process</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prvalue/">prvalue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytesseract/">pytesseract</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a><span class="tag-list-count">26</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/qt/">qt</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/quit/">quit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/re/">re</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/requests/">requests</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rvalue/">rvalue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/signal/">signal</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/">sql</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql注入/">sql注入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stl/">stl</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/">tensorflow</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/thread/">thread</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/threading/">threading</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/trie树/">trie树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/uninstall/">uninstall</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/unordered-multimap/">unordered_multimap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/upcasting/">upcasting</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vbs/">vbs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vector/">vector</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vfptr/">vfptr</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vmware/">vmware</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wait/">wait</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/xlvalue/">xlvalue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/人工智能/">人工智能</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/仿函数/">仿函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/内存模型/">内存模型</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树算法/">决策树算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分配器/">分配器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/原理/">原理</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/反斜杠/">反斜杠</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/回文字符串/">回文字符串</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多态/">多态</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/字典树/">字典树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/引用折叠/">引用折叠</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/排列组合/">排列组合</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/木马/">木马</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/模式识别/">模式识别</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/漏洞/">漏洞</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/病毒/">病毒</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/笔试/">笔试</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/缓冲区溢出/">缓冲区溢出</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机网络/">计算机网络</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贪吃蛇/">贪吃蛇</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贪心算法/">贪心算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/转载/">转载</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/迭代器/">迭代器</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/适配器/">适配器</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/1080ti/" style="font-size: 10.83px;">1080ti</a> <a href="/tags/Aggregate/" style="font-size: 10px;">Aggregate</a> <a href="/tags/Dell/" style="font-size: 11.67px;">Dell</a> <a href="/tags/Eassy/" style="font-size: 11.67px;">Eassy</a> <a href="/tags/Eval/" style="font-size: 10px;">Eval</a> <a href="/tags/Execute/" style="font-size: 10px;">Execute</a> <a href="/tags/ExecuteGlobal/" style="font-size: 10px;">ExecuteGlobal</a> <a href="/tags/Fibonacci/" style="font-size: 10px;">Fibonacci</a> <a href="/tags/IOU/" style="font-size: 10px;">IOU</a> <a href="/tags/Kolakoski/" style="font-size: 10px;">Kolakoski</a> <a href="/tags/MNIST/" style="font-size: 10.83px;">MNIST</a> <a href="/tags/MySQL/" style="font-size: 10.83px;">MySQL</a> <a href="/tags/PIL/" style="font-size: 10px;">PIL</a> <a href="/tags/POD/" style="font-size: 10px;">POD</a> <a href="/tags/QT/" style="font-size: 11.67px;">QT</a> <a href="/tags/Ubuntu/" style="font-size: 11.67px;">Ubuntu</a> <a href="/tags/YOLO/" style="font-size: 12.5px;">YOLO</a> <a href="/tags/acm/" style="font-size: 10px;">acm</a> <a href="/tags/advance/" style="font-size: 10px;">advance</a> <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/array/" style="font-size: 10px;">array</a> <a href="/tags/asp/" style="font-size: 10px;">asp</a> <a href="/tags/auto/" style="font-size: 10.83px;">auto</a> <a href="/tags/auto/" style="font-size: 10px;">auto&</a> <a href="/tags/auto/" style="font-size: 10px;">auto&&</a> <a href="/tags/binding/" style="font-size: 10px;">binding</a> <a href="/tags/c/" style="font-size: 17.5px;">c</a> <a href="/tags/c/" style="font-size: 10px;">c#</a> <a href="/tags/cin/" style="font-size: 10px;">cin</a> <a href="/tags/constexpr/" style="font-size: 10px;">constexpr</a> <a href="/tags/cpp/" style="font-size: 20px;">cpp</a> <a href="/tags/cpp标准库/" style="font-size: 17.5px;">cpp标准库</a> <a href="/tags/cpp第二版/" style="font-size: 11.67px;">cpp第二版</a> <a href="/tags/cuda/" style="font-size: 10.83px;">cuda</a> <a href="/tags/cudnn/" style="font-size: 10.83px;">cudnn</a> <a href="/tags/darknet/" style="font-size: 12.5px;">darknet</a> <a href="/tags/dd/" style="font-size: 10px;">dd</a> <a href="/tags/default/" style="font-size: 10px;">default</a> <a href="/tags/downcasting/" style="font-size: 10px;">downcasting</a> <a href="/tags/error/" style="font-size: 11.67px;">error</a> <a href="/tags/estimator/" style="font-size: 11.67px;">estimator</a> <a href="/tags/forward/" style="font-size: 10px;">forward</a> <a href="/tags/game/" style="font-size: 10px;">game</a> <a href="/tags/glvalue/" style="font-size: 10px;">glvalue</a> <a href="/tags/hook/" style="font-size: 10px;">hook</a> <a href="/tags/identity/" style="font-size: 10px;">identity</a> <a href="/tags/lambda/" style="font-size: 10px;">lambda</a> <a href="/tags/linux/" style="font-size: 16.67px;">linux</a> <a href="/tags/lvalue/" style="font-size: 10px;">lvalue</a> <a href="/tags/map/" style="font-size: 10px;">map</a> <a href="/tags/mismatch/" style="font-size: 10px;">mismatch</a> <a href="/tags/move/" style="font-size: 10px;">move</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/namehiding/" style="font-size: 10.83px;">namehiding</a> <a href="/tags/network/" style="font-size: 12.5px;">network</a> <a href="/tags/next/" style="font-size: 10px;">next</a> <a href="/tags/notes/" style="font-size: 19.17px;">notes</a> <a href="/tags/numpy/" style="font-size: 12.5px;">numpy</a> <a href="/tags/office/" style="font-size: 10.83px;">office</a> <a href="/tags/oj/" style="font-size: 10px;">oj</a> <a href="/tags/opencv/" style="font-size: 14.17px;">opencv</a> <a href="/tags/oracle/" style="font-size: 10.83px;">oracle</a> <a href="/tags/overload/" style="font-size: 10.83px;">overload</a> <a href="/tags/override/" style="font-size: 10.83px;">override</a> <a href="/tags/php/" style="font-size: 10px;">php</a> <a href="/tags/piecewise-construct/" style="font-size: 10px;">piecewise_construct</a> <a href="/tags/pip/" style="font-size: 10px;">pip</a> <a href="/tags/pipe/" style="font-size: 10.83px;">pipe</a> <a href="/tags/process/" style="font-size: 10px;">process</a> <a href="/tags/prvalue/" style="font-size: 10px;">prvalue</a> <a href="/tags/pytesseract/" style="font-size: 10px;">pytesseract</a> <a href="/tags/python/" style="font-size: 18.33px;">python</a> <a href="/tags/qt/" style="font-size: 13.33px;">qt</a> <a href="/tags/quit/" style="font-size: 10px;">quit</a> <a href="/tags/re/" style="font-size: 11.67px;">re</a> <a href="/tags/requests/" style="font-size: 10.83px;">requests</a> <a href="/tags/rvalue/" style="font-size: 10px;">rvalue</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/signal/" style="font-size: 10.83px;">signal</a> <a href="/tags/sql/" style="font-size: 10.83px;">sql</a> <a href="/tags/sql注入/" style="font-size: 10px;">sql注入</a> <a href="/tags/stl/" style="font-size: 16.67px;">stl</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/thread/" style="font-size: 10.83px;">thread</a> <a href="/tags/threading/" style="font-size: 10px;">threading</a> <a href="/tags/trie树/" style="font-size: 10px;">trie树</a> <a href="/tags/uninstall/" style="font-size: 10.83px;">uninstall</a> <a href="/tags/unordered-multimap/" style="font-size: 10px;">unordered_multimap</a> <a href="/tags/upcasting/" style="font-size: 10px;">upcasting</a> <a href="/tags/vbs/" style="font-size: 10px;">vbs</a> <a href="/tags/vector/" style="font-size: 10px;">vector</a> <a href="/tags/vfptr/" style="font-size: 10px;">vfptr</a> <a href="/tags/vmware/" style="font-size: 10px;">vmware</a> <a href="/tags/wait/" style="font-size: 10px;">wait</a> <a href="/tags/xlvalue/" style="font-size: 10px;">xlvalue</a> <a href="/tags/人工智能/" style="font-size: 10px;">人工智能</a> <a href="/tags/仿函数/" style="font-size: 10px;">仿函数</a> <a href="/tags/内存模型/" style="font-size: 10px;">内存模型</a> <a href="/tags/决策树算法/" style="font-size: 10px;">决策树算法</a> <a href="/tags/分配器/" style="font-size: 10px;">分配器</a> <a href="/tags/原理/" style="font-size: 17.5px;">原理</a> <a href="/tags/反斜杠/" style="font-size: 10px;">反斜杠</a> <a href="/tags/回文字符串/" style="font-size: 10px;">回文字符串</a> <a href="/tags/多态/" style="font-size: 10px;">多态</a> <a href="/tags/字典树/" style="font-size: 10px;">字典树</a> <a href="/tags/引用折叠/" style="font-size: 10px;">引用折叠</a> <a href="/tags/排列组合/" style="font-size: 10px;">排列组合</a> <a href="/tags/木马/" style="font-size: 10px;">木马</a> <a href="/tags/机器学习/" style="font-size: 10.83px;">机器学习</a> <a href="/tags/模式识别/" style="font-size: 10px;">模式识别</a> <a href="/tags/正则表达式/" style="font-size: 10px;">正则表达式</a> <a href="/tags/深度学习/" style="font-size: 10px;">深度学习</a> <a href="/tags/漏洞/" style="font-size: 10px;">漏洞</a> <a href="/tags/病毒/" style="font-size: 10px;">病毒</a> <a href="/tags/笔试/" style="font-size: 11.67px;">笔试</a> <a href="/tags/算法/" style="font-size: 13.33px;">算法</a> <a href="/tags/缓冲区溢出/" style="font-size: 10px;">缓冲区溢出</a> <a href="/tags/计算机网络/" style="font-size: 10.83px;">计算机网络</a> <a href="/tags/贪吃蛇/" style="font-size: 10px;">贪吃蛇</a> <a href="/tags/贪心算法/" style="font-size: 10px;">贪心算法</a> <a href="/tags/转载/" style="font-size: 15.83px;">转载</a> <a href="/tags/迭代器/" style="font-size: 12.5px;">迭代器</a> <a href="/tags/适配器/" style="font-size: 10px;">适配器</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>

    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">21</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">27</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">21</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2018 coordinate All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
